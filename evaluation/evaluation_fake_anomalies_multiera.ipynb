{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# external modules\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "# framework modules\n",
    "sys.path.append('../')\n",
    "import plotting.plottools\n",
    "importlib.reload(plotting.plottools)\n",
    "from plotting.plottools import plot_histogram\n",
    "import training.prepare_training_set\n",
    "importlib.reload(training.prepare_training_set)\n",
    "from training.prepare_training_set import prepare_training_data_from_files\n",
    "import training.patternfiltering\n",
    "importlib.reload(training.patternfiltering)\n",
    "from training.patternfiltering import contains_any_pattern\n",
    "import datagen.fake_anomaly\n",
    "importlib.reload(datagen.fake_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evaluation sets\n",
    "\n",
    "mes = ([\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+1',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+2',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+3',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-1',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-2',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-3',\n",
    "])\n",
    "\n",
    "data_eras = ([\n",
    "    'Run2023C-PromptReco-v1',\n",
    "    'Run2023C-PromptReco-v2',\n",
    "    'Run2023C-PromptReco-v3',\n",
    "    'Run2023C-PromptReco-v4',\n",
    "    'Run2023D-PromptReco-v1',\n",
    "    'Run2023D-PromptReco-v2',\n",
    "])\n",
    "\n",
    "kwargs = ({\n",
    "    'verbose': True,\n",
    "    'entries_threshold': 10000,\n",
    "    'skip_first_lumisections': 5,\n",
    "    'veto_patterns': [np.zeros((2,2)), np.zeros((2,1)), np.zeros((1,2))]\n",
    "})\n",
    "\n",
    "eval_data = {}\n",
    "for me in mes:\n",
    "    eval_data[me] = {}\n",
    "    for era in data_eras:\n",
    "        files = ['../data/data/ZeroBias-{}-DQMIO-{}_preprocessed.parquet'.format(era, me)]\n",
    "        (data, runs, lumis) = prepare_training_data_from_files(files, **kwargs)\n",
    "        eval_data[me][era] = {'data': data, 'runs': runs, 'lumis': lumis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make masks where values are often zero\n",
    "\n",
    "for me in mes:\n",
    "    for era in data_eras:\n",
    "        this_eval_data = eval_data[me][era]['data']\n",
    "        shape_mask = (np.sum(this_eval_data[:,:,:,0]==0, axis=0)>len(this_eval_data)*0.5)\n",
    "    \n",
    "        # temp: also set edges to zero\n",
    "        shape_mask[0,:] = 1\n",
    "        shape_mask[-1,:] = 1\n",
    "        shape_mask[:,0] = 1\n",
    "        shape_mask[:,-1] = 1\n",
    "    \n",
    "        eval_data[me][era]['shape_mask'] = shape_mask\n",
    "\n",
    "for era in data_eras:\n",
    "    print('Era {}'.format(era))\n",
    "    fig,axs = plt.subplots(figsize=(len(mes*6),6), ncols=len(mes))\n",
    "    if len(mes)==1: axs = [axs]\n",
    "    for i, me in enumerate(mes):\n",
    "        plot_histogram(eval_data[me][era]['shape_mask'], fig=fig, ax=axs[i], caxrange=(-0.01,1))\n",
    "        axs[i].text(0.02, 1.02, 'Shape mask for {}'.format(me.split('_ontrack_')[1]), transform=axs[i].transAxes, fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3abf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load keras models\n",
    "\n",
    "modelbase = '../models/output_20231123/model_20231123'\n",
    "model_eras = ([\n",
    "  'Run2023C-v1',\n",
    "  'Run2023C-v2',\n",
    "  'Run2023C-v3',\n",
    "  'Run2023C-v4',\n",
    "  'Run2023D-v1',\n",
    "  'Run2023D-v2',\n",
    "])\n",
    "\n",
    "models = {}\n",
    "for me in mes:\n",
    "    models[me] = {}\n",
    "    for model_era in model_eras:\n",
    "        models[me][model_era] = {}\n",
    "        modelname = '{}_{}_{}.keras'.format(modelbase, model_era, me)\n",
    "        model = keras.models.load_model(modelname)\n",
    "        models[me][model_era]['model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load average occupancy or error of training sets\n",
    "\n",
    "for me in mes:\n",
    "    for model_era in model_eras:\n",
    "        avgresponsename = '{}_{}_{}_avgoccupancy.npy'.format(modelbase, model_era, me)\n",
    "        avgresponse = np.load(avgresponsename)\n",
    "        avgresponse = np.square(avgresponse)\n",
    "        models[me][model_era]['avgresponse'] = avgresponse\n",
    "\n",
    "for model_era in model_eras:\n",
    "    print('Era {}'.format(era))\n",
    "    fig,axs = plt.subplots(figsize=(len(mes*6),6), ncols=len(mes))\n",
    "    if len(mes)==1: axs = [axs]\n",
    "    for i, me in enumerate(mes):\n",
    "        plot_histogram(models[me][model_era]['avgresponse'], fig=fig, ax=axs[i])\n",
    "        axs[i].text(0.02, 1.02, 'Average response for {}'.format(me.split('_ontrack_')[1]), transform=axs[i].transAxes, fontsize=15)\n",
    "    plt.show()\n",
    "    \n",
    "for me in mes:\n",
    "    for model_era in model_eras:\n",
    "        avgresponse = models[me][model_era]['avgresponse']\n",
    "        avgresponse[avgresponse==0] = 1\n",
    "        avgresponse = np.expand_dims(avgresponse, axis=2)\n",
    "        models[me][model_era]['avgresponse'] = avgresponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make collections of fake anomalies\n",
    "\n",
    "nanomalies = 6000\n",
    "ntimesteps = 1 # note: keep at 1 for now (i.e. no time correction), other options not yet implemented in evaluation below\n",
    "\n",
    "for me in mes:\n",
    "    for era in data_eras:\n",
    "        print('Producing anomalies for {} / {}...'.format(me, era))\n",
    "        this_eval_data = eval_data[me][era]['data']\n",
    "        this_shape_mask = eval_data[me][era]['shape_mask']\n",
    "        random_indices = np.random.choice(len(this_eval_data), size=nanomalies, replace=True)\n",
    "        random_indices[random_indices<ntimesteps-1] = ntimesteps-1\n",
    "        anomalies = np.zeros((nanomalies*ntimesteps, this_eval_data.shape[1], this_eval_data.shape[2], 1))\n",
    "        for newidx,origidx in enumerate(random_indices):\n",
    "            hist = this_eval_data[origidx-ntimesteps+1:origidx+1,:,:,0]\n",
    "            (anomalous_hist, paramdict) = datagen.fake_anomaly.dead_rectangle(hist, rectangle_min_pixels=2, shape_mask=~this_shape_mask)\n",
    "            anomalies[ntimesteps*newidx:ntimesteps*(newidx+1),:,:,0] = anomalous_hist\n",
    "        eval_data[me][era]['anomalies'] = anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ef3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which models to evaluate for which era\n",
    "\n",
    "model_era_mode = 'previous_and_current'\n",
    "\n",
    "def get_model_eras(mode='all', data_era=''):\n",
    "    if mode=='all': return model_eras\n",
    "    data_era_tag = data_era.replace('-PromptReco-', '-')\n",
    "    if mode=='previous_and_current':\n",
    "        eras = [era for era in model_eras if era<=data_era_tag]\n",
    "        if len(eras)>2: eras = eras[-2:]\n",
    "        return eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the models\n",
    "\n",
    "for me in mes:\n",
    "    for data_era in data_eras:\n",
    "        this_eval_data = eval_data[me][data_era]['data']\n",
    "        this_anomalies = eval_data[me][data_era]['anomalies']\n",
    "        this_shape_mask = eval_data[me][data_era]['shape_mask']\n",
    "        eval_data[me][data_era]['errors_time_corrected'] = {}\n",
    "        eval_data[me][data_era]['errors_anomalies_time_corrected'] = {}\n",
    "        for model_era in get_model_eras(mode=model_era_mode, data_era=data_era):    \n",
    "            this_model = models[me][model_era]['model']\n",
    "            this_avgresponse = models[me][model_era]['avgresponse']\n",
    "            print('Evaluating model for {} / {} / {}'.format(me, data_era, model_era))\n",
    "    \n",
    "            # predictions\n",
    "            predictions = this_model.predict(this_eval_data)\n",
    "            predictions[predictions<0] = 0.\n",
    "            predictions[:,this_shape_mask] = 0.\n",
    "            predictions_anomalies = this_model.predict(this_anomalies)\n",
    "            predictions_anomalies[predictions_anomalies<0] = 0.\n",
    "            predictions_anomalies[:,this_shape_mask] = 0.\n",
    "    \n",
    "            # errors\n",
    "            errors = np.square(this_eval_data - predictions)\n",
    "            errors[:,this_shape_mask] = 0.\n",
    "            errors_anomalies = np.square(this_anomalies - predictions_anomalies)\n",
    "            errors_anomalies[:,this_shape_mask] = 0.\n",
    "\n",
    "            # space correction\n",
    "            errors_space_corrected = errors/this_avgresponse\n",
    "            errors_anomalies_space_corrected = errors_anomalies/this_avgresponse\n",
    "    \n",
    "            # extra space correction: divide error by total occupancy of each lumisection\n",
    "            occupancy_perls = np.square(np.sum(this_eval_data, axis=(1,2))[:,0])\n",
    "            mean_occupancy_perls = np.mean(occupancy_perls)\n",
    "            occupancy_perls /= mean_occupancy_perls\n",
    "            errors_space_corrected = np.divide(errors_space_corrected, occupancy_perls[:,None,None,None])\n",
    "            occupancy_anomalies_perls = np.square(np.sum(this_anomalies, axis=(1,2))[:,0])\n",
    "            occupancy_anomalies_perls /= mean_occupancy_perls # use mean from non-anomalous data here for consistency\n",
    "            errors_anomalies_space_corrected = np.divide(errors_anomalies_space_corrected, occupancy_anomalies_perls[:,None,None,None])\n",
    "    \n",
    "            # time correction\n",
    "            errors_time_corrected = np.zeros(errors_space_corrected.shape)\n",
    "            for i in range(ntimesteps-1, len(errors_space_corrected)):\n",
    "                errors_time_corrected[i] = np.prod(errors_space_corrected[i-ntimesteps+1:i+1], axis=0)\n",
    "            errors_anomalies_time_corrected = np.zeros((nanomalies, this_eval_data.shape[1], this_eval_data.shape[2], this_eval_data.shape[3]))\n",
    "            for i in range(len(errors_anomalies_time_corrected)):\n",
    "                errors_anomalies_time_corrected[i:i+1] = np.prod(errors_anomalies_space_corrected[ntimesteps*i:ntimesteps*(i+1)], axis=0)\n",
    "    \n",
    "            # remove superfluous anomalies that were only introduced to be able to do the time correction\n",
    "            #this_anomalies = this_anomalies[::ntimesteps]\n",
    "            #eval_data[me]['anomalies'] = this_anomalies\n",
    "            #predictions_anomalies = predictions_anomalies[::ntimesteps]\n",
    "            #errors_anomalies = errors_anomalies[::ntimesteps]\n",
    "            #errors_anomalies_space_corrected = errors_anomalies_space_corrected[::ntimesteps]\n",
    "    \n",
    "            # add to data structure\n",
    "            eval_data[me][data_era]['errors_time_corrected'][model_era] = errors_time_corrected\n",
    "            eval_data[me][data_era]['errors_anomalies_time_corrected'][model_era] = errors_anomalies_time_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan thresholds to make ROC curves\n",
    "\n",
    "thresholds = [0.25, 0.4, 0.5, 0.6, 0.75, 1, 1.25, 1.5]\n",
    "patterns = [-np.ones((2,2)), -np.ones((2,1)), -np.ones((1,2))]\n",
    "\n",
    "for me in mes:\n",
    "    for data_era in data_eras:\n",
    "        eval_data[me][data_era]['roc'] = {}\n",
    "        for model_era in get_model_eras(mode=model_era_mode, data_era=data_era):\n",
    "            print('Making ROC curve for {} / {} / {}'.format(me, data_era, model_era))\n",
    "            errors_time_corrected = eval_data[me][data_era]['errors_time_corrected'][model_era]\n",
    "            errors_anomalies_time_corrected = eval_data[me][data_era]['errors_anomalies_time_corrected'][model_era]\n",
    "            s_eff = []\n",
    "            b_eff = []\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                print('  Threshold {} of {}'.format(i+1, len(thresholds)), end='\\r')\n",
    "                errors_scaled = np.where(errors_time_corrected>threshold, -1, errors_time_corrected)\n",
    "                errors_anomalies_scaled = np.where(errors_anomalies_time_corrected>threshold, -1, errors_anomalies_time_corrected)\n",
    "                flags = contains_any_pattern(errors_scaled[:,:,:,0], patterns)\n",
    "                flags_anomalies = contains_any_pattern(errors_anomalies_scaled[:,:,:,0], patterns)\n",
    "                #print('Threshold: {}'.format(threshold))\n",
    "                #print('{} out of {} non-anomalous histograms were tagged'.format(sum(flags), len(flags)))\n",
    "                #print('{} out of {} anomalous histograms were tagged'.format(sum(flags_anomalies), len(flags_anomalies)))\n",
    "                s_eff.append( sum(flags_anomalies)/len(flags_anomalies) )\n",
    "                b_eff.append( sum(flags)/len(flags) )\n",
    "            eval_data[me][data_era]['roc'][model_era] = (s_eff, b_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a summary plot\n",
    "\n",
    "colors = ({\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+1': 'dodgerblue',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+2': 'darkviolet',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+3': 'crimson',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-1': 'lightskyblue',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-2': 'orchid',\n",
    "    'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_-3': 'orangered',\n",
    "})\n",
    "\n",
    "# make the basic figure\n",
    "fig,axs = plt.subplots(figsize=(3*len(data_eras), 3*len(model_eras)), ncols=len(data_eras), nrows=len(model_eras), squeeze=False)\n",
    "for j, data_era in enumerate(data_eras):\n",
    "    for i, model_era in enumerate(model_eras):\n",
    "        # skip combinations of data eras and model eras that were not evaluated\n",
    "        if model_era not in get_model_eras(mode=model_era_mode, data_era=data_era):\n",
    "            axs[i,j].axis('off')\n",
    "            continue\n",
    "        # plot the lines for each ME\n",
    "        for me in mes:\n",
    "            s_eff, b_eff = eval_data[me][data_era]['roc'][model_era]\n",
    "            axs[i,j].plot(b_eff, s_eff, label=me.split('_ontrack_')[1], color=colors[me], linewidth=2)\n",
    "        # plot aesthetics\n",
    "        axs[i,j].grid()\n",
    "        axs[i,j].set_ylim((0.5,1))\n",
    "        axs[i,j].set_xlim((5e-4, 1))\n",
    "        axs[i,j].set_xscale('log')\n",
    "        # disable x- and y-axis labels when needed\n",
    "        if i!=j: axs[i,j].set_xticklabels([])\n",
    "        if j!=i: axs[i,j].set_yticklabels([])\n",
    "            \n",
    "# add the legend\n",
    "legend = axs[0,0].legend(fontsize=15, loc='upper left', bbox_to_anchor=(4, 1.))\n",
    "\n",
    "# remove space from between panels\n",
    "fig.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "# set axis titles\n",
    "for i, model_era in enumerate(model_eras):\n",
    "    axs[i,0].text(-0.2, 0.5, model_era, fontsize=12, va='center', transform=axs[i,0].transAxes, rotation=90)\n",
    "for j, data_era in enumerate(data_eras):\n",
    "    axs[-1,j].text(0.5,-0.2, data_era.replace('-PromptReco-','-'), fontsize=12, ha='center', transform=axs[-1,j].transAxes)\n",
    "fig.text(0.5, 0.05, 'Data era', ha='center', fontsize=15)\n",
    "fig.text(0.05, 0.5, 'Model era', va='center', fontsize=15, rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2229e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
