{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aad3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# external modules\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "# framework modules\n",
    "sys.path.append('../')\n",
    "import plotting.plottools\n",
    "importlib.reload(plotting.plottools)\n",
    "from plotting.plottools import plot_histogram\n",
    "import training.prepare_training_set\n",
    "importlib.reload(training.prepare_training_set)\n",
    "from training.prepare_training_set import prepare_training_data_from_files\n",
    "import training.patternfiltering\n",
    "importlib.reload(training.patternfiltering)\n",
    "from training.patternfiltering import contains_any_pattern\n",
    "import datagen.fake_anomaly\n",
    "importlib.reload(datagen.fake_anomaly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a0aa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evaluation set\n",
    "\n",
    "me = 'PixelPhase1-Tracks-PXForward-clusterposition_xy_ontrack_PXDisk_+1'\n",
    "\n",
    "files = ([\n",
    "    '../data/data/ZeroBias-Run2023C-PromptReco-v1-DQMIO-{}_preprocessed.parquet'.format(me)\n",
    "])\n",
    "kwargs = ({\n",
    "    'verbose': True,\n",
    "    'entries_threshold': 10000,\n",
    "    'skip_first_lumisections': 5,\n",
    "    'veto_patterns': [np.zeros((2,2)), np.zeros((2,1)), np.zeros((1,2))] # stricter than training set\n",
    "    #'veto_patterns': [np.zeros((2,2)), np.zeros((3,1)), np.zeros((1,3))] # same as training set\n",
    "})\n",
    "(eval_data, eval_runs, eval_lumis) = prepare_training_data_from_files(files, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d94bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mask where values are often zero\n",
    "\n",
    "shape_mask = (np.sum(eval_data[:,:,:,0]==0, axis=0)>len(eval_data)*0.5)\n",
    "\n",
    "# temp: also set edges to zero\n",
    "shape_mask[0,:] = 1\n",
    "shape_mask[-1,:] = 1\n",
    "shape_mask[:,0] = 1\n",
    "shape_mask[:,-1] = 1\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plot_histogram(shape_mask, fig=fig, ax=ax, caxrange=(-0.01,1))\n",
    "ax.text(0.02, 1.02, 'Shape mask', transform=ax.transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3abf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load keras model\n",
    "\n",
    "modelbase = '../models/output_20231123/model_20231123_Run2023C-v1'\n",
    "modelname = '{}_{}.keras'.format(modelbase, me)\n",
    "model = keras.models.load_model(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ea9ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load average occupancy or error of training set\n",
    "\n",
    "avgresponsename = '{}_{}_avgoccupancy.npy'.format(modelbase, me)\n",
    "avgresponse = np.load(avgresponsename)\n",
    "avgresponse = np.square(avgresponse)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "plot_histogram(avgresponse, fig=fig, ax=ax)\n",
    "ax.text(0.02, 1.02, 'Average response on training set', transform=ax.transAxes, fontsize=12)\n",
    "avgresponse[avgresponse==0] = 1\n",
    "avgresponse = np.expand_dims(avgresponse, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c800fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a collection of fake anomalies\n",
    "\n",
    "nanomalies = 6000\n",
    "ntimesteps = 3\n",
    "random_indices = np.random.choice(len(eval_data), size=nanomalies, replace=True)\n",
    "random_indices[random_indices<ntimesteps-1] = ntimesteps-1\n",
    "\n",
    "anomalies = np.zeros((nanomalies*ntimesteps, eval_data.shape[1], eval_data.shape[2], 1))\n",
    "params = []\n",
    "for newidx,origidx in enumerate(random_indices):\n",
    "    hist = eval_data[origidx-ntimesteps+1:origidx+1,:,:,0]\n",
    "    (anomalous_hist, paramdict) = datagen.fake_anomaly.dead_rectangle(hist, rectangle_min_pixels=2, shape_mask=~shape_mask)\n",
    "    #(anomalous_hist, paramdict) = datagen.fake_anomaly.dead_sector(hist)\n",
    "    #(anomalous_hist, paramdict) = datagen.fake_anomaly.hot_rectangle(hist, shape_mask=~shape_mask)\n",
    "    #(anomalous_hist, paramdict) = datagen.fake_anomaly.hot_sector(hist)\n",
    "    anomalies[ntimesteps*newidx:ntimesteps*(newidx+1),:,:,0] = anomalous_hist\n",
    "    params.append(paramdict)\n",
    "print('Shape of anomalies array: {}'.format(anomalies.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "predictions = model.predict(eval_data)\n",
    "predictions[predictions<0] = 0.\n",
    "predictions[:,shape_mask] = 0.\n",
    "\n",
    "predictions_anomalies = model.predict(anomalies)\n",
    "predictions_anomalies[predictions_anomalies<0] = 0.\n",
    "predictions_anomalies[:,shape_mask] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0785040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate squared difference\n",
    "errors = np.square(eval_data - predictions)\n",
    "errors[:,shape_mask] = 0.\n",
    "errors_anomalies = np.square(anomalies - predictions_anomalies)\n",
    "errors_anomalies[:,shape_mask] = 0.\n",
    "\n",
    "# space correction\n",
    "errors_space_corrected = errors/avgresponse\n",
    "errors_anomalies_space_corrected = errors_anomalies/avgresponse\n",
    "\n",
    "# extra space correction: divide by average occupancy of each lumisection\n",
    "occupancy_perls = np.square(np.sum(eval_data, axis=(1,2))[:,0])\n",
    "mean_occupancy_perls = np.mean(occupancy_perls)\n",
    "occupancy_perls /= mean_occupancy_perls\n",
    "errors_space_corrected = np.divide(errors_space_corrected, occupancy_perls[:,None,None,None])\n",
    "occupancy_anomalies_perls = np.square(np.sum(anomalies, axis=(1,2))[:,0])\n",
    "occupancy_anomalies_perls /= mean_occupancy_perls # use mean from non-anomalous data here for consistency\n",
    "errors_anomalies_space_corrected = np.divide(errors_anomalies_space_corrected, occupancy_anomalies_perls[:,None,None,None])\n",
    "\n",
    "# time correction\n",
    "errors_time_corrected = np.zeros(errors_space_corrected.shape)\n",
    "for i in range(ntimesteps-1, len(errors_space_corrected)):\n",
    "    errors_time_corrected[i] = np.prod(errors_space_corrected[i-ntimesteps+1:i+1], axis=0)\n",
    "errors_anomalies_time_corrected = np.zeros((nanomalies, eval_data.shape[1], eval_data.shape[2], eval_data.shape[3]))\n",
    "for i in range(len(errors_anomalies_time_corrected)):\n",
    "    errors_anomalies_time_corrected[i:i+1] = np.prod(errors_anomalies_space_corrected[ntimesteps*i:ntimesteps*(i+1)], axis=0)\n",
    "    \n",
    "# remove superfluous anomalies that were only introduced to be able to do the time correction\n",
    "anomalies = anomalies[::ntimesteps]\n",
    "predictions_anomalies = predictions_anomalies[::ntimesteps]\n",
    "errors_anomalies = errors_anomalies[::ntimesteps]\n",
    "errors_anomalies_space_corrected = errors_anomalies_space_corrected[::ntimesteps]\n",
    "print(anomalies.shape)\n",
    "print(predictions_anomalies.shape)\n",
    "print(errors_anomalies.shape)\n",
    "print(errors_anomalies_space_corrected.shape)\n",
    "print(errors_anomalies_time_corrected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bceddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots of evaluation on supposedly non-anomalous histograms\n",
    "\n",
    "nplots = 5\n",
    "plotids = np.random.choice(len(eval_data), size=nplots, replace=False)\n",
    "\n",
    "for i in plotids:\n",
    "    fig,axs = plt.subplots(figsize=(30,6), ncols=5)\n",
    "    plot_histogram(eval_data[i,:,:,0], fig=fig, ax=axs[0])\n",
    "    plot_histogram(predictions[i,:,:,0], fig=fig, ax=axs[1])\n",
    "    plot_histogram(errors[i,:,:,0], fig=fig, ax=axs[2], caxrange=(-0.001, 0.01))\n",
    "    plot_histogram(errors_space_corrected[i,:,:,0], fig=fig, ax=axs[3], caxrange=(-0.01, 0.5))\n",
    "    plot_histogram(errors_time_corrected[i,:,:,0], fig=fig, ax=axs[4], caxrange=(-0.01, 0.5))\n",
    "    axs[0].text(0.02, 1.02, 'Run {}, LS {}'.format(eval_runs[i], eval_lumis[i]), transform=axs[0].transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b105025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make plots of evaluation on anomalous histograms\n",
    "\n",
    "nplots = 5\n",
    "plotids = np.random.choice(len(anomalies), size=nplots, replace=False)\n",
    "\n",
    "for i in plotids:\n",
    "    fig,axs = plt.subplots(figsize=(30,6), ncols=5)\n",
    "    plot_histogram(anomalies[i,:,:,0], fig=fig, ax=axs[0])\n",
    "    plot_histogram(predictions_anomalies[i,:,:,0], fig=fig, ax=axs[1])\n",
    "    plot_histogram(errors_anomalies[i,:,:,0], fig=fig, ax=axs[2], caxrange=(-0.001, 0.01))\n",
    "    plot_histogram(errors_anomalies_space_corrected[i,:,:,0], fig=fig, ax=axs[3], caxrange=(-0.01, 0.5))\n",
    "    plot_histogram(errors_anomalies_time_corrected[i,:,:,0], fig=fig, ax=axs[4], caxrange=(-0.01, 0.5))\n",
    "    axs[0].text(0.02, 1.02, 'Fake anomaly', transform=axs[0].transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4dfc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan thresholds to make a ROC curve\n",
    "\n",
    "#thresholds = [0.1, 0.25, 0.4, 0.5, 0.6, 0.75, 1, 1.25, 1.5]\n",
    "thresholds = np.logspace(-4 ,0, num=30)\n",
    "patterns = [-np.ones((2,2)), -np.ones((2,1)), -np.ones((1,2))]\n",
    "\n",
    "s_eff = []\n",
    "b_eff = []\n",
    "for threshold in thresholds:\n",
    "    errors_scaled = np.where(errors_time_corrected>threshold, -1, errors_time_corrected)\n",
    "    errors_anomalies_scaled = np.where(errors_anomalies_time_corrected>threshold, -1, errors_anomalies_time_corrected)\n",
    "    flags = contains_any_pattern(errors_scaled[:,:,:,0], patterns)\n",
    "    flags_anomalies = contains_any_pattern(errors_anomalies_scaled[:,:,:,0], patterns)\n",
    "    print('Threshold: {}'.format(threshold))\n",
    "    print('{} out of {} non-anomalous histograms were tagged'.format(sum(flags), len(flags)))\n",
    "    print('{} out of {} anomalous histograms were tagged'.format(sum(flags_anomalies), len(flags_anomalies)))\n",
    "    s_eff.append( sum(flags_anomalies)/len(flags_anomalies) )\n",
    "    b_eff.append( sum(flags)/len(flags) )\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(b_eff, s_eff, c='b', s=10)\n",
    "ax.set_xlabel('False alarm rate', fontsize=15)\n",
    "ax.set_ylabel('True anomaly efficiency', fontsize=15)\n",
    "ax.grid()\n",
    "#ax.set_ylim((0,1))\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914dc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "ax.scatter(b_eff, s_eff, c='b', s=10)\n",
    "ax.set_xlabel('False alarm rate', fontsize=15)\n",
    "ax.set_ylabel('True anomaly efficiency', fontsize=15)\n",
    "ax.grid()\n",
    "#ax.set_ylim((0,1))\n",
    "ax.set_xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given threshold, investigate mistagged good histograms\n",
    "\n",
    "nplots = 5\n",
    "threshold = 0.75\n",
    "errors_scaled = np.where(errors_time_corrected>threshold, -1, errors_time_corrected)\n",
    "flags = contains_any_pattern(errors_scaled[:,:,:,0], patterns)\n",
    "print('Found {} candidates, of which plotting {}'.format(sum(flags), nplots))\n",
    "plotids = np.random.choice(np.arange(len(eval_data))[flags], size=nplots, replace=False)\n",
    "\n",
    "for i in plotids:\n",
    "    fig,axs = plt.subplots(figsize=(30,6), ncols=5)\n",
    "    plot_histogram(eval_data[i,:,:,0], fig=fig, ax=axs[0])\n",
    "    plot_histogram(predictions[i,:,:,0], fig=fig, ax=axs[1])\n",
    "    plot_histogram(errors[i,:,:,0], fig=fig, ax=axs[2], caxrange=(-0.001, 0.01))\n",
    "    plot_histogram(errors_space_corrected[i,:,:,0], fig=fig, ax=axs[3], caxrange=(-0.01, threshold))\n",
    "    plot_histogram(errors_time_corrected[i,:,:,0], fig=fig, ax=axs[4], caxrange=(-0.01, threshold))\n",
    "    axs[0].text(0.02, 1.02, 'Run {}, LS {}'.format(eval_runs[i], eval_lumis[i]), transform=axs[0].transAxes, fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9addfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given threshold, investigate missed anomalous histograms\n",
    "\n",
    "nplots = 5\n",
    "threshold = 0.75\n",
    "errors_anomalies_scaled = np.where(errors_anomalies_time_corrected>threshold, -1, errors_anomalies_time_corrected)\n",
    "flags_anomalies = contains_any_pattern(errors_anomalies_scaled[:,:,:,0], patterns)\n",
    "print('Found {} candidates, of which plotting {}'.format(sum(~flags_anomalies), nplots))\n",
    "plotids = np.random.choice(np.arange(len(anomalies))[~flags_anomalies], size=nplots, replace=False)\n",
    "\n",
    "for i in plotids:\n",
    "    fig,axs = plt.subplots(figsize=(30,6), ncols=5)\n",
    "    plot_histogram(anomalies[i,:,:,0], fig=fig, ax=axs[0])\n",
    "    plot_histogram(predictions_anomalies[i,:,:,0], fig=fig, ax=axs[1])\n",
    "    plot_histogram(errors_anomalies[i,:,:,0], fig=fig, ax=axs[2], caxrange=(-0.001, 0.01))\n",
    "    plot_histogram(errors_anomalies_space_corrected[i,:,:,0], fig=fig, ax=axs[3], caxrange=(-0.01, threshold))\n",
    "    plot_histogram(errors_anomalies_time_corrected[i,:,:,0], fig=fig, ax=axs[4], caxrange=(-0.01, threshold))\n",
    "    axs[0].text(0.02, 1.02, 'Fake anomaly', transform=axs[0].transAxes, fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8293656b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
