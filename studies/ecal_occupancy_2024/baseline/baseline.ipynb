{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# external modules\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from fnmatch import fnmatch\n",
    "from functools import partial\n",
    "import importlib\n",
    "\n",
    "# local modules\n",
    "thisdir = os.getcwd()\n",
    "topdir = os.path.abspath(os.path.join(thisdir, '../../../'))\n",
    "sys.path.append(topdir)\n",
    "\n",
    "import tools.iotools as iotools\n",
    "import tools.dftools as dftools\n",
    "import tools.patternfiltering as patternfiltering\n",
    "from tools.dataloadertools import MEDataLoader\n",
    "import studies.ecal_occupancy_2024.preprocessing.preprocessor\n",
    "importlib.reload(studies.ecal_occupancy_2024.preprocessing.preprocessor)\n",
    "from studies.ecal_occupancy_2024.preprocessing.preprocessor import PreProcessor, make_default_preprocessor\n",
    "from studies.ecal_occupancy_2024.plotting.plot_cluster_occupancy import plot_cluster_occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07cb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "\n",
    "# settings\n",
    "datadir = '/eos/user/l/llambrec/dialstools-output-test'\n",
    "year = '2024'\n",
    "eras = [\n",
    "    'B-v1',\n",
    "    'C-v1',\n",
    "    'D-v1',\n",
    "    'E-v1',\n",
    "    'E-v2',\n",
    "    'F-v1',\n",
    "    'G-v1',\n",
    "    'H-v1',\n",
    "    'I-v1',\n",
    "    'I-v2'\n",
    "]\n",
    "dataset = 'ZeroBias'\n",
    "reco = 'PromptReco'\n",
    "menames = {\n",
    "    'EB': 'EcalBarrel-EBOccupancyTask-EBOT digi occupancy',\n",
    "    #'EE+': 'EcalEndcap-EEOccupancyTask-EEOT digi occupancy EE +',\n",
    "    #'EE-': 'EcalEndcap-EEOccupancyTask-EEOT digi occupancy EE -'\n",
    "}\n",
    "\n",
    "# find files corresponding to settings\n",
    "files = {}\n",
    "for era in eras:\n",
    "    files[era] = {}\n",
    "    for melabel, mename in menames.items():\n",
    "        mainera, version = era.split('-', 1)\n",
    "        f = f'{dataset}-Run{year}{mainera}-{reco}-{version}-DQMIO-{mename}.parquet'\n",
    "        f = os.path.join(datadir, f)\n",
    "        files[era][melabel] = f\n",
    "\n",
    "# existence check\n",
    "missing = []\n",
    "for era, temp in files.items():\n",
    "    for melabel, f in temp.items():\n",
    "        if not os.path.exists(f):\n",
    "            missing.append(f)\n",
    "if len(missing) > 0:\n",
    "    raise Exception(f'The following files do not exist: {missing}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e19d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to OMS files\n",
    "\n",
    "omsfiles = {}\n",
    "for era in eras:\n",
    "    basedir = '../omsdata'\n",
    "    omsfile = os.path.join(basedir, f'omsdata_Run{year}{era}.json')\n",
    "    if not os.path.exists(omsfile):\n",
    "        print(f'WARNING: OMS file {omsfile} does not exist.')\n",
    "    omsfiles[era] = omsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e8de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to HLT rate files\n",
    "# (just use the ones from pixel study as we're dealing with the same ZeroBias triggers)\n",
    "\n",
    "hltfiles = {}\n",
    "for era in eras:\n",
    "    basedir = '../../pixel_clusters_2024/omsdata'\n",
    "    hltfile = os.path.join(basedir, f'hltrate_Run{year}{era}.json')\n",
    "    if not os.path.exists(hltfile):\n",
    "        print(f'WARNING: HLT rate file {hltfile} does not exist.')\n",
    "    hltfiles[era] = hltfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de9e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load total number of lumisections per era\n",
    "\n",
    "nlumis = {}\n",
    "melabel = list(menames.keys())[0]\n",
    "for era in eras:\n",
    "    dftemp = iotools.read_parquet(files[era][melabel], columns=['run_number'])\n",
    "    nlumis[era] = len(dftemp)\n",
    "nlumis['total'] = sum(nlumis.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from studies.pixel_clusters_2024.nmf.nmf_testing_pattern import filter_dfs # make local equivalent later\n",
    "\n",
    "# loop over mes\n",
    "results = {}\n",
    "for melabel, mename in menames.items():\n",
    "    \n",
    "    # initialize results\n",
    "    results[melabel] = {}\n",
    "    \n",
    "    # get shape of current ME\n",
    "    temp = PreProcessor(melabel)\n",
    "    meshape = temp.required_dims\n",
    "    \n",
    "    # initialize loss maps to prepend to each batch\n",
    "    # (for the dynamic loss map and time correction to not reset between batches)\n",
    "    prepend_size = 10 # must be larger than dynamic loss map and time correction size, maybe later determine automatically\n",
    "    prepend_binary_loss_batch = np.zeros((prepend_size, meshape[0], meshape[1]))\n",
    "    \n",
    "    # loop over eras\n",
    "    for era in eras:\n",
    "        print(f'Now processing me {mename}, era {era}...')\n",
    "        \n",
    "        # make dataloader\n",
    "        dataloader = MEDataLoader([files[era][melabel]])\n",
    "        preprocessor_era = 'C-v1'\n",
    "        preprocessor = make_default_preprocessor(preprocessor_era, melabel,\n",
    "                     global_normalization = 'avg',\n",
    "                     local_normalization = 'avg')\n",
    "        \n",
    "        # make (static) loss mask\n",
    "        loss_mask_era = 'C-v1'\n",
    "        loss_mask_file = f'../preprocessing/normdata/zerofrac_Run2024{loss_mask_era}_{melabel}.npy'\n",
    "        loss_mask = np.load(loss_mask_file)\n",
    "        loss_mask = (loss_mask < 0.9).astype(bool)\n",
    "        total_loss_mask = np.copy(loss_mask)\n",
    "        \n",
    "        # other initializations\n",
    "        flagged_run_numbers = []\n",
    "        flagged_ls_numbers = []\n",
    "        batch_filter_results = []\n",
    "        batch_txt_results = []\n",
    "        \n",
    "        # load OMS data\n",
    "        with open(omsfiles[era]) as f:\n",
    "            oms_info = json.load(f)\n",
    "            \n",
    "        # load HLT rate data\n",
    "        with open(hltfiles[era]) as f:\n",
    "            hltrate_info = json.load(f)\n",
    "        \n",
    "        # run over batches\n",
    "        batch_size = 20000\n",
    "        batch_params = dataloader.prepare_sequential_batches(batch_size=batch_size)\n",
    "        print(f'Will process {len(batch_params)} batches of size {batch_size}')\n",
    "        for batch_idx, batch_param in enumerate(batch_params):\n",
    "            batch = dataloader.read_sequential_batch(batch_param)\n",
    "            batch_size = len(batch)\n",
    "            print(f'  Loaded batch {batch_idx+1} / {len(batch_params)} with size {batch_size}')\n",
    "            \n",
    "            # filtering\n",
    "            fdict = {melabel: batch}\n",
    "            oms_filters = [\n",
    "                ['beams_stable'],\n",
    "                ['cms_active'],\n",
    "                ['ebm_ready'],\n",
    "                ['ebp_ready'],\n",
    "                ['eem_ready'],\n",
    "                ['eep_ready'],\n",
    "                ['esm_ready'],\n",
    "                ['esp_ready'],\n",
    "                ['pileup', '>', 10]\n",
    "            ]\n",
    "            hltrate_filters = [\n",
    "                [\"HLT_ZeroBias_v*\", '>', 25]\n",
    "            ]\n",
    "            mask, filter_results = filter_dfs(fdict,\n",
    "                                     oms_info = oms_info,\n",
    "                                     oms_filters = oms_filters,\n",
    "                                     hltrate_info = hltrate_info,\n",
    "                                     hltrate_filters = hltrate_filters\n",
    "                                    )\n",
    "            mask = mask.astype(bool)\n",
    "            batch = batch[mask]\n",
    "            batch_size_filtered = len(batch)\n",
    "            print(f'  Found {batch_size_filtered} / {batch_size} instances passing filters.')\n",
    "            if batch_size_filtered==0:\n",
    "                # stop further processing (except adding filter results)\n",
    "                batch_filter_results.append(filter_results)\n",
    "                continue\n",
    "                \n",
    "            # update, hard-coded for now, to be generalized later:\n",
    "            # get a more fine-grained local normalization.\n",
    "            # note: just a rough first version, taking the proper normalization for the first run in the batch\n",
    "            #       and assume it is valid for the whole batch; this is fine in most cases on the condition\n",
    "            #       that the evaluation batch size is the same or much smaller than the batches used to make the normalization.\n",
    "            first_run = int(batch['run_number'].values[0])\n",
    "            candidates = sorted([f for f in os.listdir(f'../preprocessing/normdata/') if fnmatch(f, f'avgme_*_{melabel}_*')])\n",
    "            norm_idx = -1\n",
    "            for candidate_idx, candidate in enumerate(candidates):\n",
    "                last_run = int(candidate.split('run')[-1].split('-')[-1].replace('.npy', ''))\n",
    "                if last_run >= first_run:\n",
    "                    norm_idx = candidate_idx - 1\n",
    "                    break\n",
    "            if norm_idx < 0: norm_idx = 0\n",
    "            normfile = candidates[norm_idx]\n",
    "            normfile = os.path.join('../preprocessing/normdata', normfile)\n",
    "            normmap = np.load(normfile)\n",
    "            normmap[normmap==0] = -1\n",
    "            preprocessor.local_norm = normmap\n",
    "            \n",
    "            # preprocessing\n",
    "            mes_preprocessed = preprocessor.preprocess(batch)\n",
    "            \n",
    "            # flagging criterion: look for empty regions\n",
    "            #patterns = {\n",
    "            #    'EB': [\n",
    "            #        np.zeros((2,2))\n",
    "            #    ],\n",
    "            #    'EE+': [np.zeros((2,2))\n",
    "            #     ],\n",
    "            #    'EE-': [\n",
    "            #        np.zeros((2,2))\n",
    "            #    ]\n",
    "            #}\n",
    "            #flag_empty_regions = patternfiltering.contains_any_pattern(mes_preprocessed, patterns[melabel], mask=loss_mask).astype(bool)\n",
    "            \n",
    "            # look for regions different from the reference\n",
    "            # (for reference we use just 1 and for the difference we use the ratio, so in practice nothing needs to be done,\n",
    "            #  just define thresholds on the preprocessed MEs)\n",
    "            binary_loss = np.logical_or(mes_preprocessed < 0.1, mes_preprocessed > 5)\n",
    "            \n",
    "            # prepend losses from the previous batch\n",
    "            binary_loss = np.concatenate((prepend_binary_loss_batch, binary_loss))\n",
    "            prepend_binary_loss_batch = binary_loss[-prepend_size:, :, :]\n",
    "            \n",
    "            # define dynamic loss map\n",
    "            # (i.e. mask out bins that have been lossy for the past n LS)\n",
    "            half_window_size = 5\n",
    "            window = np.concatenate((np.zeros(half_window_size+1), np.ones(half_window_size))).astype(float)\n",
    "            window /= np.sum(window)\n",
    "            dynamic_loss_mask = sp.ndimage.convolve1d(binary_loss.astype(float), window, axis=0)\n",
    "            dynamic_loss_mask = (dynamic_loss_mask >= 0.999)\n",
    "            total_loss_mask = ((loss_mask) & (~dynamic_loss_mask))\n",
    "                \n",
    "            # do time correction\n",
    "            half_window_size = 3\n",
    "            window = np.concatenate((np.zeros(half_window_size-1), np.ones(half_window_size))).astype(float)\n",
    "            window /= np.sum(window)\n",
    "            binary_loss_time_corrected = sp.ndimage.convolve1d(binary_loss.astype(float), window, axis=0)\n",
    "            binary_loss_time_corrected = (binary_loss_time_corrected >= 0.999)\n",
    "            \n",
    "            # flag patterns\n",
    "            patterns = {\n",
    "                'EB': [\n",
    "                    np.ones((1,1))\n",
    "                ],\n",
    "                'EE+': [\n",
    "                    np.ones((1,1))\n",
    "                ],\n",
    "                'EE-': [\n",
    "                    np.ones((1,1))\n",
    "                ]\n",
    "            }\n",
    "            flag_extrema = patternfiltering.contains_any_pattern(binary_loss_time_corrected, patterns[melabel], mask=total_loss_mask).astype(bool)\n",
    "            flag_extrema = flag_extrema[prepend_size:]\n",
    "            \n",
    "            # optional: parse loss map to txt (e.g. for later processing with language models...)\n",
    "            final_loss_map = np.multiply(binary_loss_time_corrected, total_loss_mask)[prepend_size:]\n",
    "            ids = np.nonzero(final_loss_map)\n",
    "            time_ids = ids[0]\n",
    "            spatial_ids = (ids[1], ids[2])\n",
    "            run_nbs = batch['run_number'].values[time_ids]\n",
    "            ls_nbs = batch['ls_number'].values[time_ids]\n",
    "            for idx in range(len(time_ids)):\n",
    "                infostr = f'ME {melabel}, era {era}, run {run_nbs[idx]}, LS {ls_nbs[idx]}: bin {spatial_ids[0][idx]}, {spatial_ids[1][idx]}'\n",
    "                batch_txt_results.append(infostr)\n",
    "            \n",
    "            # combine criteria\n",
    "            flags = (\n",
    "                #(flag_empty_regions) |\n",
    "                (flag_extrema)\n",
    "            )\n",
    "            \n",
    "            # store flagged lumisections\n",
    "            flagged_run_numbers_batch = batch['run_number'].values[flags]\n",
    "            flagged_ls_numbers_batch = batch['ls_number'].values[flags]\n",
    "            n_unique_runs = len(np.unique(flagged_run_numbers_batch))\n",
    "            print(f'    -> Found {np.sum(flags.astype(int))} flagged lumisections in {n_unique_runs} runs.')\n",
    "            \n",
    "            # add to results\n",
    "            flagged_run_numbers.append(flagged_run_numbers_batch)\n",
    "            flagged_ls_numbers.append(flagged_ls_numbers_batch)\n",
    "            batch_filter_results.append(filter_results)\n",
    "            \n",
    "        # contatenate the results from the batches\n",
    "        filter_results = {}\n",
    "        if len(batch_filter_results)>0:\n",
    "            for key in batch_filter_results[0].keys():\n",
    "                filter_results[key] = sum([batch_filter_result[key] for batch_filter_result in batch_filter_results], [])\n",
    "        if len(flagged_run_numbers) > 0:\n",
    "            flagged_run_numbers = np.concatenate(flagged_run_numbers)\n",
    "            flagged_ls_numbers = np.concatenate(flagged_ls_numbers)\n",
    "        else:\n",
    "            flagged_run_numbers = np.array([])\n",
    "            flagged_ls_numbers = np.array([])\n",
    "        \n",
    "        # add to dict\n",
    "        results[melabel][era] = {\n",
    "            'flagged_run_numbers': flagged_run_numbers,\n",
    "            'flagged_ls_numbers': flagged_ls_numbers,\n",
    "            'filter_results': filter_results\n",
    "        }\n",
    "        \n",
    "        # optional: write txt output\n",
    "        outputfile = f'temp_{melabel}_{era}_flagged_bins.txt'\n",
    "        with open(outputfile, 'w') as f:\n",
    "            for line in batch_txt_results:\n",
    "                f.write(line+'\\n')\n",
    "                \n",
    "    # optional: concatenate txt output files\n",
    "    cmd = f'cat temp_{melabel}_*_flagged_bins.txt > temp_{melabel}_flagged_bins.txt'\n",
    "    os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232b99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print flagged lumisections\n",
    "\n",
    "nflags = {}\n",
    "ntotal_ls = 0\n",
    "ntotal_runs = 0\n",
    "for melabel, mename in menames.items():\n",
    "    print(f'Results for {melabel}:')\n",
    "    nflags[melabel] = {}\n",
    "    for era in eras:\n",
    "        flagged_run_numbers = results[melabel][era]['flagged_run_numbers']\n",
    "        flagged_ls_numbers = results[melabel][era]['flagged_ls_numbers']\n",
    "        print(f'  Flagged lumisections in era {era} ({len(flagged_run_numbers)}):')\n",
    "        for run, ls in zip(flagged_run_numbers, flagged_ls_numbers):\n",
    "            print(f'    Run {run}, LS {ls}')\n",
    "        ntotal_ls += len(flagged_ls_numbers)\n",
    "        ntotal_runs += len(np.unique(flagged_run_numbers))\n",
    "        nflags[melabel][era] = len(flagged_ls_numbers)\n",
    "print('---')\n",
    "print('Results for total:')\n",
    "print(f'  Flagged {ntotal_ls} lumisections in {ntotal_runs} runs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7886e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the filter results\n",
    "\n",
    "do_plot_filter = True\n",
    "\n",
    "if do_plot_filter:\n",
    "    \n",
    "    # help functions for plotting\n",
    "    def abs_to_frac(x, tot=1):\n",
    "        return x / tot\n",
    "\n",
    "    def frac_to_abs(x, tot=1):\n",
    "        return x * tot\n",
    "    \n",
    "    melabel = list(menames.keys())[0]\n",
    "    nfiltered = {}\n",
    "    for era in eras:\n",
    "            filter_results = results[melabel][era]['filter_results']\n",
    "\n",
    "            # make a table\n",
    "            filter_results_arrays = {key: np.array([el[0]*10000+el[1] for el in val]) for key, val in filter_results.items()}\n",
    "            failed_ls = np.unique(np.concatenate(list(filter_results_arrays.values())))\n",
    "            nfiltered[era] = {key: len(val) for key, val in filter_results_arrays.items()}\n",
    "            nfiltered[era]['total'] = len(failed_ls)\n",
    "\n",
    "            # make a figure\n",
    "            #fig, ax = plt.subplots(figsize=(8, 4))\n",
    "            #ax.bar(nfiltered[era].keys(), nfiltered[era].values())\n",
    "            #ax.set_xticks(ax.get_xticks())\n",
    "            #ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=15)\n",
    "            #ax.set_ylabel('Number of failing LS', fontsize=15)\n",
    "            #ax.grid(which='both', axis='y', color='gray', linestyle='dashed')\n",
    "            #ax.text(0, 1.03, f'Lumisection preselection for era {era}', transform=ax.transAxes, fontsize=15)\n",
    "            #plt.show()\n",
    "            \n",
    "    # combined\n",
    "    nfiltered['total'] = {}\n",
    "    for key in nfiltered[eras[0]]:\n",
    "        nfiltered['total'][key] = sum([nfiltered[era][key] for era in eras])\n",
    "    \n",
    "    # make a figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    labels = list(nfiltered['total'].keys())\n",
    "    labels = [label.replace('>', '$>$') for label in labels]\n",
    "    ax.bar(labels, nfiltered['total'].values())\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=15)\n",
    "    ax.set_ylabel('Number of failing LS', fontsize=15)\n",
    "    ax.grid(which='both', axis='y', color='gray', linestyle='dashed')\n",
    "    ax.text(0, 1.03, f'Lumisection preselection', transform=ax.transAxes, fontsize=15)\n",
    "    secyax = ax.secondary_yaxis('right', functions=(partial(abs_to_frac, tot=nlumis['total']), partial(frac_to_abs, tot=nlumis['total'])))\n",
    "    secyax.set_ylabel('Fraction of failed LS', fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b17d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print fraction of flagged LS per era\n",
    "\n",
    "tot_nlumis = 0\n",
    "tot_npass = 0\n",
    "tot_nflags = 0\n",
    "melabel = list(menames.keys())[0]\n",
    "for era in eras:\n",
    "    this_nlumis = nlumis[era]\n",
    "    this_npass = this_nlumis - nfiltered[era]['total']\n",
    "    this_nflags = nflags[melabel][era]\n",
    "    tot_nlumis += this_nlumis\n",
    "    tot_npass += this_npass\n",
    "    tot_nflags += this_nflags\n",
    "    print(f'Era {era}:')\n",
    "    print(f'Flagged {this_nflags} out of {this_nlumis} lumisections in total' + ' ({:.2f} %)'.format(this_nflags/this_nlumis*100))\n",
    "    print(f'Flagged {this_nflags} out of {this_npass} lumisections that pass filters' + ' ({:.2f} %)'.format(this_nflags/this_npass*100))\n",
    "    \n",
    "print('-----')\n",
    "print(f'Total:')\n",
    "print(f'Flagged {tot_nflags} out of {tot_nlumis} lumisections in total' + ' ({:.2f} %)'.format(tot_nflags/tot_nlumis*100))\n",
    "print(f'Flagged {tot_nflags} out of {tot_npass} lumisections that pass filters' + ' ({:.2f} %)'.format(tot_nflags/tot_npass*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out why a given lumisection did not pass the selections\n",
    "\n",
    "test_runlumis = [(386795, 37), (386795, 38), (386795, 39), (386795, 40), (386795, 41), (386795, 42), (386795, 43)]\n",
    "for runlumi in test_runlumis:\n",
    "    failkeys = []\n",
    "    for melabel, mename in menames.items():\n",
    "        for era in eras:\n",
    "            filter_results = results[melabel][era]['filter_results']\n",
    "            \n",
    "            for key, values in filter_results.items():\n",
    "                if tuple(runlumi) in values: failkeys.append(key)\n",
    "    if len(failkeys)==0:\n",
    "        print(f'Lumisection {runlumi} not found in filter info, i.e. it did not seem to have failed any of the selections.')\n",
    "    else:\n",
    "        print(f'Lumisection {runlumi} failed the following selections:')\n",
    "        print(failkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for plotting some random (or not random) examples\n",
    "\n",
    "# random lumisections\n",
    "nplot = 0\n",
    "#random_ids = np.random.choice(len(available_run_numbers), size=min(nplot, len(available_run_numbers)), replace=False)\n",
    "#selected_run_numbers = available_run_numbers[random_ids]\n",
    "#selected_ls_numbers = available_ls_numbers[random_ids]\n",
    "#random_ids = np.random.choice(len(flagged_run_numbers), size=min(nplot, len(flagged_run_numbers)), replace=False)\n",
    "#selected_run_numbers = flagged_run_numbers[random_ids]\n",
    "#selected_ls_numbers = flagged_ls_numbers[random_ids]\n",
    "\n",
    "# alternative: specific selected lumisections\n",
    "era = 'I-v2'\n",
    "selected_runlumis = [(386851, 97)]\n",
    "selected_run_numbers = [el[0] for el in selected_runlumis]\n",
    "selected_ls_numbers = [el[1] for el in selected_runlumis]\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # calculate random indices and load data\n",
    "    print('Loading data...')\n",
    "    dfs = {}\n",
    "    mes = {}\n",
    "    for melabel, mename in menames.items():\n",
    "        dfs[melabel] = iotools.read_lumisections(files[era][melabel], selected_run_numbers, selected_ls_numbers, mode='batched')\n",
    "        mes[melabel], runs, lumis = dftools.get_mes(dfs[melabel], xbinscolumn='x_bin', ybinscolumn='y_bin', runcolumn='run_number', lumicolumn='ls_number')\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb01832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples\n",
    "\n",
    "import studies.ecal_occupancy_2024.plotting.plot_cluster_occupancy\n",
    "importlib.reload(studies.ecal_occupancy_2024.plotting.plot_cluster_occupancy)\n",
    "from studies.ecal_occupancy_2024.plotting.plot_cluster_occupancy import plot_cluster_occupancy\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # make loss mask\n",
    "    loss_mask_era = 'C-v1'\n",
    "    loss_mask_file = f'../preprocessing/normdata/zerofrac_Run2024{loss_mask_era}_{melabel}.npy'\n",
    "    loss_mask = np.load(loss_mask_file)\n",
    "    loss_mask = (loss_mask < 0.9).astype(bool)\n",
    "    \n",
    "    # preprocess\n",
    "    print('Processing...')\n",
    "    mes_preprocessed = {}\n",
    "    mes_pred = {}\n",
    "    losses = {}\n",
    "    for melabel, mename in mes.items():\n",
    "        preprocessor_era = 'C-v1'\n",
    "        preprocessor = make_default_preprocessor(preprocessor_era, melabel,\n",
    "                         global_normalization = 'avg',\n",
    "                         local_normalization = 'avg')\n",
    "        mes_preprocessed[melabel] = preprocessor.preprocess(dfs[melabel])\n",
    "        \n",
    "    # calculate binary loss\n",
    "    binary_losses = {}\n",
    "    for melabel, mename in mes.items():\n",
    "        binary_losses[melabel] = np.logical_or(mes_preprocessed[melabel] < 0.1, mes_preprocessed[melabel] > 5)\n",
    "        \n",
    "    # do (static) loss masking\n",
    "    binary_losses_static_masked = {}\n",
    "    for melabel, mename in mes.items():\n",
    "        binary_losses_static_masked[melabel] = np.multiply(binary_losses[melabel], loss_mask[np.newaxis, :, :])\n",
    "        \n",
    "    # make the plots\n",
    "    print('Plotting...')\n",
    "    for idx in range(len(selected_run_numbers)):\n",
    "        run = runs[idx]\n",
    "        lumi = lumis[idx]\n",
    "        for melabel, mename in menames.items():\n",
    "            me_orig = mes[melabel][idx, :, :]\n",
    "            me_preprocessed = mes_preprocessed[melabel][idx, :, :]\n",
    "            binary_loss = binary_losses[melabel][idx, :, :]\n",
    "            binary_loss_static_masked = binary_losses_static_masked[melabel][idx, :, :]\n",
    "    \n",
    "            # initialize figure\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(18, 6), squeeze=False)\n",
    "            \n",
    "            # plot raw data\n",
    "            fig, axs[0, 0] = plot_cluster_occupancy(me_orig, fig=fig, ax=axs[0, 0],\n",
    "                   title='Raw', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Digi occupancy',\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "        \n",
    "            # plot preprocessed\n",
    "            fig, axs[0, 1] = plot_cluster_occupancy(me_preprocessed, fig=fig, ax=axs[0, 1],\n",
    "                   title='Input', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Digi occupancy\\n(normalized)',\n",
    "                   caxrange=(1e-6,5),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            \n",
    "            # plot binary loss\n",
    "            fig, axs[0, 2] = plot_cluster_occupancy(binary_loss, fig=fig, ax=axs[0, 2],\n",
    "                   title='Binary loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(1e-6, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            \n",
    "            # plot binary loss (with static loss mask)\n",
    "            fig, axs[1, 0] = plot_cluster_occupancy(binary_loss_static_masked, fig=fig, ax=axs[1, 0],\n",
    "                   title='Binary loss (with mask applied)', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(1e-6, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "                \n",
    "            # plot aesthetics\n",
    "            plt.subplots_adjust(hspace=0.5, wspace=-0.1)\n",
    "            fig.tight_layout()\n",
    "            title = f'Run {run}, LS {lumi}, ECAL {melabel}'\n",
    "            axs[0, 0].text(0.01, 1.3, title, fontsize=15, transform=axs[0, 0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
