{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import copy\n",
    "import importlib\n",
    "import numpy as np\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "topdir = os.path.abspath(os.path.join(thisdir, '../../../'))\n",
    "sys.path.append(topdir)\n",
    "\n",
    "from studies.pixel_clusters_2024.preprocessing.preprocessor import get_metype\n",
    "from studies.pixel_clusters_2024.nmf.nmf_training import find_files\n",
    "import studies.pixel_clusters_2024.nmf.nmf_testing_pattern as evaltools\n",
    "importlib.reload(evaltools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96398157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# era and layer settings\n",
    "\n",
    "eras = [\n",
    "    #'2024B-v1',\n",
    "    #'2024C-v1',\n",
    "    #'2024D-v1',\n",
    "    #'2024E-v1',\n",
    "    #'2024E-v2',\n",
    "    #'2024F-v1',\n",
    "    #'2024F-v1-part1',\n",
    "    #'2024F-v1-part2',\n",
    "    #'2024F-v1-part3',\n",
    "    #'2024F-v1-part4',\n",
    "    #'2024G-v1',\n",
    "    #'2024G-v1-part1',\n",
    "    #'2024G-v1-part2',\n",
    "    #'2024G-v1-part3',\n",
    "    #'2024G-v1-part4',\n",
    "    #'2024H-v1',\n",
    "    #'2024I-v1',\n",
    "    #'2024I-v2',\n",
    "    #'2025B-v1',\n",
    "    '2025C-v1',\n",
    "    #'2025C-v2',\n",
    "    #'2025D-v1',\n",
    "    #'2025E-v1',\n",
    "    #'2025F-v1',\n",
    "    #'2025F-v2',\n",
    "    #'2025G-v1'\n",
    "]\n",
    "layers = [\n",
    "    'BPix1',\n",
    "    'BPix2',\n",
    "    'BPix3',\n",
    "    'BPix4'\n",
    "]\n",
    "\n",
    "# initialize config\n",
    "config = {}\n",
    "config['eras'] = eras\n",
    "config['layers'] = layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to input files\n",
    "\n",
    "inputmode = 'dials'\n",
    "#dataset = 'ZeroBias'\n",
    "#reco = 'PromptReco'\n",
    "dataset = 'StreamExpress'\n",
    "reco = 'Express'\n",
    "mebase = 'PixelPhase1-Phase1_MechanicalView-PXBarrel-clusters_per_SignedModuleCoord_per_SignedLadderCoord_PXLayer_{}'\n",
    "\n",
    "# default case of using stored input files\n",
    "if inputmode=='filebased':\n",
    "\n",
    "    # settings\n",
    "    datadir = '/eos/user/l/llambrec/dialstools-output'\n",
    "    npart = None # not yet defined for file-based input\n",
    "\n",
    "    # find files corresponding to settings\n",
    "    input_files = {}\n",
    "    for era in eras:\n",
    "        mainera, version = era.split('-', 1)\n",
    "        input_files[era] = {}\n",
    "        for layer in layers:\n",
    "            me = mebase.format(layer[-1])\n",
    "            f = f'{dataset}-Run{mainera}-{reco}-{version}-DQMIO-{me}.parquet'\n",
    "            f = os.path.join(datadir, f)\n",
    "            input_files[era][layer] = [f]\n",
    "\n",
    "    # existence check\n",
    "    missing = []\n",
    "    present = []\n",
    "    for era, values in input_files.items():\n",
    "        for layer, files in values.items():\n",
    "            for f in files:\n",
    "                if not os.path.exists(f): missing.append(f)\n",
    "                else: present.append(f)\n",
    "    if len(missing) > 0:\n",
    "        raise Exception(f'The following files do not exist: {missing}')\n",
    "    else: print(f'Found {len(present)} files.')\n",
    "\n",
    "# alternative mode of getting input on the fly from dials\n",
    "# (experimental)\n",
    "elif inputmode=='dials':\n",
    "\n",
    "    # settings\n",
    "    nparts = 10\n",
    "    \n",
    "    # make dials filters corresponding to settings\n",
    "    input_files = {}\n",
    "    for era in eras:\n",
    "        mainera, version = era.split('-', 1)\n",
    "        input_files[era] = {}\n",
    "        for part in range(nparts):\n",
    "            input_files[era][part] = {}\n",
    "            for layer in layers:\n",
    "                me = mebase.format(layer[-1])\n",
    "                dfilter = {\n",
    "                    'dataset': f'/{dataset}/Run{mainera}-{reco}-{version}/DQMIO',\n",
    "                    'me': me.replace('-', '/'),\n",
    "                    'part': part,\n",
    "                    'nparts': nparts\n",
    "                }\n",
    "                input_files[era][part][layer] = [dfilter]\n",
    "\n",
    "# add to config\n",
    "config['input_files'] = input_files\n",
    "print(json.dumps(input_files, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13525e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to nmf model files\n",
    "\n",
    "modeldir = 'output_20250714_consolidation/models'\n",
    "\n",
    "# set up how to match eras\n",
    "# options:\n",
    "# - current era for each input file (e.g. for offline application)\n",
    "# - previous era for each input file (e.g. for online application)\n",
    "# - one and the same era for each input file\n",
    "\n",
    "# use current era\n",
    "#model_eradict = {era: era for era in eras}\n",
    "\n",
    "# use previous era\n",
    "#model_eradict = {}\n",
    "#for i in range(len(eras)): model_eradict[eras[i]] = eras[i-1]\n",
    "#model_eradict[eras[0]] = eras[1]\n",
    "\n",
    "# use a fixed era\n",
    "model_eradict = {era: 'C-v1' for era in eras}\n",
    "\n",
    "# set path\n",
    "nmf_files = {}\n",
    "for era in eras:\n",
    "    nmf_files[era] = {}\n",
    "    model_era = model_eradict[era]\n",
    "    for layer in layers: nmf_files[era][layer] = os.path.join(modeldir, f'nmf_model_{layer.upper()}_{model_era}.pkl')\n",
    "    \n",
    "# existence check\n",
    "missing = []\n",
    "for era in eras:\n",
    "    for layer, f in nmf_files[era].items():\n",
    "        if not os.path.exists(f): missing.append(f)\n",
    "if len(missing) > 0:\n",
    "    raise Exception(f'The following files do not exist: {missing}')\n",
    "    \n",
    "# add to config\n",
    "config['nmf_files'] = nmf_files\n",
    "print(json.dumps(nmf_files, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e59cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for preprocessing\n",
    "\n",
    "local_norm_eradict = {}\n",
    "for era in eras: local_norm_eradict[era] = 'avg_era_2024C-v1' if '2024' in era else 'avg_era_2025B-v1'\n",
    "#for era in eras: local_norm_eradict[era] = 'avg_era_2024C-v1'\n",
    "    \n",
    "# add to config\n",
    "config['preprocessing_global_normalization'] = 'avg'\n",
    "config['preprocessing_local_normalization'] = local_norm_eradict\n",
    "print(json.dumps(local_norm_eradict, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb88750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for filtering\n",
    "\n",
    "# min entries filter\n",
    "min_entries_filter = {\n",
    "    'BPix1': 0.5e6,\n",
    "    'BPix2': 0.5e6/2,\n",
    "    'BPix3': 0.5e6/3,\n",
    "    'BPix4': 0.5e6/4\n",
    "}\n",
    "\n",
    "# set OMS filters\n",
    "oms_filters = [\n",
    "    [\"beams_stable\"],\n",
    "    [\"cms_active\"],\n",
    "    [\"bpix_ready\"],\n",
    "    [\"fpix_ready\"],\n",
    "    [\"tibtid_ready\"],\n",
    "    [\"tob_ready\"],\n",
    "    [\"tecp_ready\"],\n",
    "    [\"tecm_ready\"],\n",
    "    [\"pileup\", '>', 25]\n",
    "]\n",
    "\n",
    "# set path to files with OMS info\n",
    "oms_filter_files = {}\n",
    "for era in eras:\n",
    "    oms_era = era\n",
    "    if '-part' in era: oms_era = era.split('-part')[0]\n",
    "    #oms_filter_files[era] = f'/eos/user/l/llambrec/pixelae/studies/pixel_clusters_2024/omsdata/omsdata_Run{oms_era}.json'\n",
    "    oms_filter_files[era] = None # retrieve on the fly\n",
    "for oms_filter_file in oms_filter_files.values():\n",
    "    if oms_filter_file is None: continue\n",
    "    if not os.path.exists(oms_filter_file):\n",
    "        raise Exception(f'File {oms_filter_file} does not exist.')\n",
    "\n",
    "# set HLT rate filters\n",
    "hltrate_filters = [\n",
    "    [\"HLT_ZeroBias_v*\", '>', 5]\n",
    "]\n",
    "\n",
    "# set path to files with HLT rate info\n",
    "hltrate_filter_files = {}\n",
    "for era in eras:\n",
    "    hltrate_era = era\n",
    "    if '-part' in era: hltrate_era = era.split('-part')[0]\n",
    "    #hltrate_filter_files[era] = f'/eos/user/l/llambrec/pixelae/studies/pixel_clusters_2024/omsdata/hltrate_Run{hltrate_era}.json'\n",
    "    hltrate_filter_files[era] = None # retrieve on the fly\n",
    "for hltrate_filter_file in hltrate_filter_files.values():\n",
    "    if hltrate_filter_file is None: continue\n",
    "    if not os.path.exists(hltrate_filter_file):\n",
    "        raise Exception(f'File {hltrate_filter_file} does not exist.')\n",
    "\n",
    "# add to config\n",
    "config['min_entries_filter'] = min_entries_filter\n",
    "config['oms_filter_files'] = oms_filter_files\n",
    "config['oms_filters'] = oms_filters\n",
    "config['hltrate_filter_files'] = hltrate_filter_files\n",
    "config['hltrate_filters'] = hltrate_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f3830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to automask data\n",
    "\n",
    "do_automasking = False\n",
    "\n",
    "automask_data_file = None\n",
    "if do_automasking:\n",
    "    automask_data_dir = '/eos/user/l/llambrec/pixelae/automasking/data/automask_data'\n",
    "    automask_data_file = os.path.join(automask_data_dir, f'automask_2024.json')\n",
    "    if not os.path.exists(automask_data_file):\n",
    "        raise Exception(f'The automask data file {automask_data_file} does not exist.')\n",
    "    config['automask_data_file'] = automask_data_file\n",
    "    \n",
    "# add to config\n",
    "config['do_automasking'] = do_automasking\n",
    "config['automask_data_file'] = automask_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to loss mask data\n",
    "\n",
    "do_loss_masking = True\n",
    "loss_masking_zero_frac_threshold = 0.9\n",
    "\n",
    "# set up how to match eras\n",
    "# options:\n",
    "# - current era for each input file (e.g. for offline application)\n",
    "# - previous era for each input file (e.g. for online application)\n",
    "# - one and the same era for each input file\n",
    "\n",
    "# use current era\n",
    "#loss_mask_eradict = {era: era for era in eras}\n",
    "\n",
    "# use previous era\n",
    "#loss_mask_eradict = {}\n",
    "#for i in range(len(eras)): loss_mask_eradict[eras[i]] = eras[i-1]\n",
    "#loss_mask_eradict[eras[0]] = eras[1]\n",
    "\n",
    "# use a fixed era\n",
    "loss_mask_eradict = {}\n",
    "for era in eras: loss_mask_eradict[era] = '2024C-v1' if '2024' in era else '2025B-v1'\n",
    "\n",
    "# set path\n",
    "loss_masking_zero_frac_files = None\n",
    "if do_loss_masking:\n",
    "    loss_masking_zero_frac_files = {}\n",
    "    for era in eras:\n",
    "        loss_mask_era = era\n",
    "        if '-part' in era: loss_mask_era = era.split('-part')[0]\n",
    "        loss_mask_era = loss_mask_eradict[loss_mask_era]\n",
    "        loss_masking_zero_frac_files[era] = {}\n",
    "        for layer in layers:\n",
    "            zerofrac_file = f'/eos/user/l/llambrec/pixelae/studies/pixel_clusters_2024/preprocessing/normdata/zerofrac_Run{loss_mask_era}_{get_metype(layer)}.npy'\n",
    "            loss_masking_zero_frac_files[era][layer] = zerofrac_file\n",
    "\n",
    "# existence check\n",
    "missing = []\n",
    "for era in eras:\n",
    "    for layer, f in loss_masking_zero_frac_files[era].items():\n",
    "        if not os.path.exists(f): missing.append(f)\n",
    "if len(missing) > 0:\n",
    "    raise Exception(f'The following files do not exist: {missing}')\n",
    "    \n",
    "# add to config\n",
    "config['do_loss_masking'] = do_loss_masking\n",
    "config['loss_masking_zero_frac_threshold'] = loss_masking_zero_frac_threshold\n",
    "config['loss_masking_zero_frac_files'] = loss_masking_zero_frac_files\n",
    "print(json.dumps(loss_masking_zero_frac_files, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53cdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for dynamic loss masking\n",
    "\n",
    "do_dynamic_loss_masking = False\n",
    "dynamic_loss_masking_window = 100\n",
    "\n",
    "config['do_dynamic_loss_masking'] = do_dynamic_loss_masking\n",
    "config['dynamic_loss_masking_window'] = dynamic_loss_masking_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set evaluation parameters\n",
    "\n",
    "# general\n",
    "batch_size = 'run' # either an int (for fixed size batches) or \"run\" (for run-based batches)\n",
    "target_batch_size = 3000 # ignored for fixed size batches, approximate batch size for run-based batches\n",
    "\n",
    "# flagging\n",
    "flagging_patterns = [np.ones((1,8)), np.ones((2,4))]\n",
    "flagging_threshold = 1e-3\n",
    "\n",
    "# clustering\n",
    "pattern_thresholds = [\n",
    "    {\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5},\n",
    "    #{\"loss_threshold\": 0.01, \"pattern\": np.ones((2, 24)).tolist(), \"filter_threshold\": 1.5},\n",
    "    #{\"loss_threshold\": 0.005, \"pattern\": np.ones((4, 24)).tolist(), \"filter_threshold\": 3.5},\n",
    "    #{\"loss_threshold\": 0.002, \"pattern\": np.ones((8, 24)).tolist(), \"filter_threshold\": 3.5},\n",
    "]\n",
    "\n",
    "# add to config\n",
    "config['batch_size'] = batch_size\n",
    "config['target_batch_size'] = target_batch_size\n",
    "config['flagging_patterns'] = [el.tolist() for el in flagging_patterns]\n",
    "config['flagging_threshold'] = flagging_threshold\n",
    "config['pattern_thresholds'] = pattern_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b13096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set directory for this config\n",
    "\n",
    "outputdir = 'output_test_part3'\n",
    "\n",
    "if os.path.exists(outputdir):\n",
    "    msg = f'Output directory {outputdir} already exists.'\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d381f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output file\n",
    "\n",
    "outputfile = os.path.join(outputdir, 'flagged_lumisections.json')\n",
    "outputfile = os.path.join('/eos/user/l/llambrec/pixelae/studies/pixel_clusters_2024/nmf', outputfile)\n",
    "# note: cannot use getwcd() to define the directory above,\n",
    "#       as it seems to give some kind of virtual directory that is ok in the notebook\n",
    "#       but gives errors when used in the lxplus terminal or condor job.\n",
    "\n",
    "config['outputfile'] = outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc34976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the configuration\n",
    "\n",
    "split_per_era = True\n",
    "basename = 'temp_config'\n",
    "\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)\n",
    "\n",
    "if split_per_era:\n",
    "    for era in eras:\n",
    "        for part in range(nparts): # todo: fix for file-based input\n",
    "            this_config = copy.deepcopy(config)\n",
    "            this_config['eras'] = [era]\n",
    "            this_config['input_files'] = {era: input_files[era][part]}\n",
    "            this_config['nmf_files'] = {era: nmf_files[era]}\n",
    "            this_config['oms_filter_files'] = {era: oms_filter_files[era]}\n",
    "            this_config['hltrate_filter_files'] = {era: hltrate_filter_files[era]}\n",
    "            this_config['outputfile'] = outputfile.replace('.json', f'_{era}_part{part}.json')\n",
    "            if this_config['loss_masking_zero_frac_files'] is not None:\n",
    "                this_config['loss_masking_zero_frac_files'] = {era: loss_masking_zero_frac_files[era]}\n",
    "            configfile = os.path.join(outputdir, f'{basename}_{era}_part{part}.json')\n",
    "            with open(configfile, 'w') as f:\n",
    "                json.dump(this_config, f, indent=2)\n",
    "\n",
    "else:\n",
    "    configfile = os.path.join(outputdir, f'{basename}.json')\n",
    "    with open(configfile, 'w') as f:\n",
    "        json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3d7e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
