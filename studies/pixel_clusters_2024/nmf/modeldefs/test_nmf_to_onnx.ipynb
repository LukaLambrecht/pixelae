{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431e18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install converter package\n",
    "# see https://onnx.ai/sklearn-onnx/introduction.html\n",
    "\n",
    "#!pip install --user skl2onnx\n",
    "\n",
    "# after installation, need to restart the session and make sure to tick 'use python packages installed in CERNBox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general imports\n",
    "\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "rng = np.random.default_rng()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import MiniBatchNMF\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a64f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some toy data\n",
    "\n",
    "data_shape = 50\n",
    "n_instances = 20\n",
    "xax = np.linspace(0, 1, num=data_shape)\n",
    "y = xax\n",
    "noise = rng.normal(scale=0.2*y, size=(n_instances, data_shape))\n",
    "X_train = y + noise\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "\n",
    "# make a plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(n_instances): ax.plot(xax, X_train[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ad6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an NMF model\n",
    "\n",
    "# make NMF\n",
    "nmf = MiniBatchNMF(\n",
    "  n_components = 3,\n",
    "  batch_size = 5\n",
    ")\n",
    "\n",
    "# fit to data\n",
    "_ = nmf.fit(X_train)\n",
    "#nmf.components_ = normalize(nmf.components_, axis=1)\n",
    "# (note: the normalization step above appears to improve things if no approximation steps are used,\n",
    "#        but seems to be not needed with sufficient approximation steps.)\n",
    "\n",
    "# plot components\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(nmf.n_components_): ax.plot(xax, nmf.components_[i, :])\n",
    "ax.text(0, 1.02, 'NMF components', ha='left', va='bottom', transform=ax.transAxes)\n",
    "\n",
    "# get transformed and reverse-transformed data for comparison later\n",
    "X_trans = nmf.transform(X_train)\n",
    "X_pred = nmf.inverse_transform(nmf.transform(X_train))\n",
    "\n",
    "# plot an example\n",
    "fig, ax = plt.subplots()\n",
    "idx = 0\n",
    "ax.plot(xax, X_train[idx, :], color='dodgerblue', label='Orig')\n",
    "ax.plot(xax, X_pred[idx, :], color='darkorchid', label='Pred')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9819f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to define and register custom converters\n",
    "# see https://onnx.ai/sklearn-onnx/auto_tutorial/plot_icustom_converter.html\n",
    "# and chatGPT...\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skl2onnx import update_registered_converter\n",
    "from skl2onnx.algebra.onnx_ops import OnnxMatMul, OnnxRelu, OnnxAdd, OnnxDiv, OnnxMul\n",
    "from onnx import onnx_pb as onnx_proto\n",
    "\n",
    "class NMFTransformWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # dummy since model is supposed to be already trained\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.inverse_transform(self.model.transform(X))\n",
    "\n",
    "def nmf_transform_shape_calculator(operator):\n",
    "    input_type = operator.inputs[0].type\n",
    "    n_features = input_type.shape[1]\n",
    "    n_components = operator.raw_operator.model.n_components\n",
    "    operator.outputs[0].type = FloatTensorType([None, n_features])\n",
    "\n",
    "def nmf_transform_converter(scope, operator, container):\n",
    "    \n",
    "    # set number of approximation steps (hard-coded for now)\n",
    "    n_approx_steps = 50\n",
    "    \n",
    "    # get raw model\n",
    "    model = operator.raw_operator.model\n",
    "    \n",
    "    # get input and output names\n",
    "    input_name = operator.inputs[0]\n",
    "    output_name = operator.outputs[0].full_name\n",
    "    \n",
    "    # get the model components (H) and their transpose\n",
    "    # H is of shape (n_components, n_features)\n",
    "    # H_T is of shape (n_features, n_components)\n",
    "    H = model.components_.astype(np.float32)\n",
    "    H_T = H.T\n",
    "    H_name = scope.get_unique_variable_name(\"H\")\n",
    "    H_T_name = scope.get_unique_variable_name(\"H_T\")\n",
    "    container.add_initializer(H_name,\n",
    "                              onnx_proto.TensorProto.FLOAT,\n",
    "                              H.shape,\n",
    "                              H.flatten())\n",
    "    container.add_initializer(H_T_name,\n",
    "                              onnx_proto.TensorProto.FLOAT,\n",
    "                              H_T.shape,\n",
    "                              H_T.flatten())\n",
    "    \n",
    "    # also get H times H_T, the epsilon parameter, and X times H_T,\n",
    "    # needed for the approximation of sklearns internal least squares solver\n",
    "    if n_approx_steps > 0:\n",
    "        HH_T = np.matmul(H, H_T)\n",
    "        eps = np.array([1e-8], dtype=np.float32)\n",
    "        HH_T_name = scope.get_unique_variable_name(\"HH_T\")\n",
    "        eps_name = scope.get_unique_variable_name(\"eps\")\n",
    "        container.add_initializer(HH_T_name, onnx_proto.TensorProto.FLOAT, HH_T.shape, HH_T.flatten())\n",
    "        container.add_initializer(eps_name, onnx_proto.TensorProto.FLOAT, [1], eps)\n",
    "        XH_T_name = scope.get_unique_variable_name(\"XH_T\")\n",
    "        XH_T = OnnxMatMul(input_name, H_T_name, output_names=[XH_T_name], op_version=container.target_opset)\n",
    "        XH_T.add_to(scope, container)\n",
    "\n",
    "    # Step 1: W_pre = ReLU(X @ H^T)\n",
    "    # matrix dimensons: (n_instances, n_features)\n",
    "    #                   * (n_features, n_components)\n",
    "    #                   = (n_instances, n_components)\n",
    "    W0_raw = scope.get_unique_variable_name(\"W0_raw\")\n",
    "    W0 = scope.get_unique_variable_name(\"W0\")\n",
    "\n",
    "    matmul1 = OnnxMatMul(operator.inputs[0], H_T_name,\n",
    "                         output_names=[W0_raw],\n",
    "                         op_version=container.target_opset)\n",
    "\n",
    "    relu = OnnxRelu(matmul1,\n",
    "                    output_names=[W0],\n",
    "                    op_version=container.target_opset)\n",
    "    W_current = W0\n",
    "    \n",
    "    # Step 1b: improve the approximation (optional)\n",
    "    for stepidx in range(n_approx_steps):\n",
    "        denom = scope.get_unique_variable_name(f\"denom_{stepidx}\")\n",
    "        denom_eps = scope.get_unique_variable_name(f\"denom_eps_{stepidx}\")\n",
    "        frac = scope.get_unique_variable_name(f\"frac_{stepidx}\")\n",
    "        W_next = scope.get_unique_variable_name(f\"W_{stepidx}\")\n",
    "        \n",
    "        matmul_denom = OnnxMatMul(W_current, HH_T_name, output_names=[denom], op_version=container.target_opset)\n",
    "        add_eps = OnnxAdd(denom, eps_name, output_names=[denom_eps], op_version=container.target_opset)\n",
    "        div = OnnxDiv(XH_T_name, denom_eps, output_names=[frac], op_version=container.target_opset)\n",
    "        update = OnnxMul(W_current, frac, output_names=[W_next], op_version=container.target_opset)\n",
    "        \n",
    "        W_current = W_next\n",
    "        \n",
    "        for node in [matmul_denom, add_eps, div, update]:\n",
    "            node.add_to(scope, container)\n",
    "\n",
    "    # Step 2: XÌ‚ = W @ H\n",
    "    # matrix dimensions: (n_instances, n_components)\n",
    "    #                    * (n_components, n_features)\n",
    "    #                    = (n_instances, n_features)\n",
    "    recon_out = operator.outputs[0].full_name\n",
    "\n",
    "    matmul2 = OnnxMatMul(W_current, H_name,\n",
    "                         output_names=[recon_out],\n",
    "                         op_version=container.target_opset)\n",
    "\n",
    "    # Add nodes to container\n",
    "    for node in [matmul1, relu, matmul2]:\n",
    "        node.add_to(scope, container)\n",
    "\n",
    "# Register the converter for our wrapper\n",
    "converter = nmf_transform_converter\n",
    "update_registered_converter(\n",
    "    NMFTransformWrapper, \"NMFTransformWrapper\",\n",
    "    nmf_transform_shape_calculator, converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert model to ONNX\n",
    "\n",
    "initial_type = [\n",
    "    ('input', FloatTensorType([None, X_train.shape[1]])),\n",
    "]\n",
    "nmf_wrapped = NMFTransformWrapper(nmf)\n",
    "nmf_onnx = convert_sklearn(nmf_wrapped, initial_types=initial_type)\n",
    "with open('test.onnx', \"wb\") as f:\n",
    "    f.write(nmf_onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb616a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ONNX model and run inference\n",
    "\n",
    "import onnxruntime as rt\n",
    "\n",
    "session = rt.InferenceSession(\"test.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "input_name = session.get_inputs()[0].name\n",
    "label_name = session.get_outputs()[0].name\n",
    "pred_onnx = session.run([label_name], {input_name: X_train.astype(np.float32)})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d07127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare direct transform with detour via ONNX\n",
    "\n",
    "idx = 0\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xax, X_train[idx, :], color='dodgerblue', label='Orig')\n",
    "ax.plot(xax, X_pred[idx, :], color='darkorchid', label='Pred')\n",
    "ax.plot(xax, pred_onnx[idx, :], color='mediumvioletred', linestyle='--', label='Pred (onnx)')\n",
    "ax.legend()\n",
    "\n",
    "# same but normalized\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xax, X_train[idx, :]/np.sum(X_train[idx, :]), color='dodgerblue', label='Orig')\n",
    "ax.plot(xax, X_pred[idx, :]/np.sum(X_pred[idx, :]), color='darkorchid', label='Pred')\n",
    "ax.plot(xax, pred_onnx[idx, :]/np.sum(pred_onnx[idx, :]), color='mediumvioletred', linestyle='--', label='Pred (onnx)')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610f86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
