{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3bc931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import joblib\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "topdir = os.path.abspath(os.path.join(thisdir, '../../../'))\n",
    "sys.path.append(topdir)\n",
    "\n",
    "import tools.iotools as iotools\n",
    "import tools.dftools as dftools\n",
    "import tools.patternfiltering as patternfiltering\n",
    "import tools.rebinning as rebinning\n",
    "import tools.clustering as clustering\n",
    "import plotting.plottools as plottools\n",
    "from automasking.tools.automaskreader import AutomaskReader\n",
    "from studies.pixel_clusters_2024.preprocessing.preprocessor import PreProcessor\n",
    "from studies.pixel_clusters_2024.preprocessing.preprocessor import get_metype\n",
    "from studies.pixel_clusters_2024.plotting.plot_cluster_occupancy import plot_cluster_occupancy\n",
    "from studies.pixel_clusters_2024.nmf.nmf_testing_pattern import make_preprocessors\n",
    "\n",
    "# for using dial for data retrieval on the fly\n",
    "#from cmsdials import Dials\n",
    "#from cmsdials.filters import LumisectionHistogram2DFilters, RunFilters\n",
    "from studies.pixel_clusters_2024.nmf.nmf_testing_pattern import load_nmfs\n",
    "from studies.pixel_clusters_2024.nmf.nmf_testing_pattern import get_dials_creds, get_data_from_dials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2feba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set era\n",
    "\n",
    "era = '2024G-v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config file\n",
    "\n",
    "#config_file = None\n",
    "config_file = f'output_20250714_consolidation/output_baseline/temp_config_{era.replace(\"2024\", \"\")}.json'\n",
    "#config_file = f'output_20250911_2025data/output_baseline/temp_config_{era}.json'\n",
    "#config_file = f'output_20250911_2025data/output_tightbpix1/temp_config_{era}_part0.json'\n",
    "\n",
    "if config_file is not None:\n",
    "    \n",
    "    # read config file\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    # read eras and layers\n",
    "    eras = config['eras']\n",
    "    layers = config['layers']\n",
    "    print(f'Found following eras: {eras}')\n",
    "    print(f'Found following layers: {layers}')\n",
    "    \n",
    "    # read input files\n",
    "    input_files = config['input_files']\n",
    "\n",
    "    # make preprocessors and models\n",
    "    print('Making preprocessors...')\n",
    "    global_normalization = config.get('preprocessing_global_normalization', None)\n",
    "    local_normalization = config.get('preprocessing_local_normalization', None)\n",
    "    local_normalization = {era: local_normalization.replace('era_', 'era_2024') for era in eras} # older convention\n",
    "    preprocessors = make_preprocessors(eras, layers,\n",
    "                      global_normalization = global_normalization,\n",
    "                      local_normalization = local_normalization)\n",
    "    print('Loading NMFs...')\n",
    "    nmfs = load_nmfs(config['nmf_files'])\n",
    "    \n",
    "    # get evaluation settings\n",
    "    batch_size = config['batch_size']\n",
    "    flagging_patterns = [np.array(el) for el in config['flagging_patterns']]\n",
    "    flagging_threshold = config['flagging_threshold']\n",
    "    #pattern_thresholds = config['pattern_thresholds']\n",
    "    \n",
    "    # for reading files in older convention\n",
    "    pattern_thresholds = {\n",
    "    'BPix1': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "    'BPix2': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "    'BPix3': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "    'BPix4': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}]\n",
    "    }\n",
    "    \n",
    "    do_automasking = config['do_automasking']\n",
    "    do_loss_masking = config['do_loss_masking']\n",
    "\n",
    "    # make automask reader if needed\n",
    "    automask_reader = None\n",
    "    automask_map_preprocessors = None\n",
    "    if do_automasking:\n",
    "        automask_reader = AutomaskReader(config['automask_data_file'])\n",
    "        automask_map_preprocessors = {}\n",
    "        for layer in layers: automask_map_preprocessors[layer] = PreProcessor(f'PXLayer_{layer}')\n",
    "\n",
    "    # make loss mask if needed\n",
    "    loss_masks = None\n",
    "    loss_mask_preprocessors = None\n",
    "    if do_loss_masking:\n",
    "        loss_masks = {}\n",
    "        loss_mask_preprocessors = {}\n",
    "        for era in eras:\n",
    "            loss_masks[era] = {}\n",
    "            for layer in layers:\n",
    "                loss_mask_file = config['loss_masking_zero_frac_files'][era][layer]\n",
    "                loss_mask_file = loss_mask_file.replace('clusters_2024', 'pixel_clusters_2024') # older convention\n",
    "                loss_mask = np.load(loss_mask_file)\n",
    "                loss_mask = (loss_mask < config['loss_masking_zero_frac_threshold'])\n",
    "                loss_masks[era][layer] = loss_mask\n",
    "        for layer in layers: loss_mask_preprocessors[layer] = PreProcessor(f'PXLayer_{layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f300547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to input files\n",
    "\n",
    "do_change_input_files = False\n",
    "\n",
    "if do_change_input_files:\n",
    "    print('Changing input files...')\n",
    "    \n",
    "    # settings\n",
    "    datadir = '/eos/user/l/llambrec/dialstools-output'\n",
    "    dataset = 'ZeroBias'\n",
    "    reco = 'PromptReco'\n",
    "    mebase = 'PixelPhase1-Phase1_MechanicalView-PXBarrel-'\n",
    "    mebase += 'clusters_per_SignedModuleCoord_per_SignedLadderCoord_PXLayer_'\n",
    "\n",
    "    # find files corresponding to settings\n",
    "    input_files = {}\n",
    "    mainera, version = era.split('-', 1)\n",
    "    input_files[era] = {}\n",
    "    for layer in layers:\n",
    "        f = f'{dataset}-Run{mainera}-{reco}-{version}-DQMIO-{mebase}{layer[-1]}.parquet'\n",
    "        f = os.path.join(datadir, f)\n",
    "        input_files[era][layer] = [f]\n",
    "    \n",
    "# existence check\n",
    "missing = []\n",
    "present = []\n",
    "for era, values in input_files.items():\n",
    "    for layer, files in values.items():\n",
    "        for f in files:\n",
    "            if not isinstance(f, str): continue\n",
    "            if not os.path.exists(f): missing.append(f)\n",
    "            else: present.append(f)\n",
    "if len(missing) > 0:\n",
    "    raise Exception(f'The following files do not exist: {missing}')\n",
    "\n",
    "# print for checking and debugging\n",
    "print('Found following input files:')\n",
    "print(json.dumps(input_files, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82d0968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nmf models\n",
    "\n",
    "do_change_model = False\n",
    "\n",
    "if do_change_model:\n",
    "    print('Changing model configuration...')\n",
    "    \n",
    "    # set model directory\n",
    "    modeldir = 'output_20250604/models'\n",
    "\n",
    "    # set up how to match eras\n",
    "    # options:\n",
    "    # - current era for each input file (e.g. for offline application)\n",
    "    # - previous era for each input file (e.g. for online application)\n",
    "\n",
    "    # use current era\n",
    "    #model_eradict = {era: era}\n",
    "\n",
    "    # use previous era\n",
    "    '''model_eradict = {\n",
    "      'B-v1': 'C-v1',\n",
    "      'C-v1': 'B-v1',\n",
    "      'D-v1': 'C-v1',\n",
    "      'E-v1': 'D-v1',\n",
    "      'E-v2': 'E-v1',\n",
    "      'F-v1': 'E-v2',\n",
    "      'G-v1': 'F-v1',\n",
    "      'H-v1': 'G-v1',\n",
    "      'I-v1': 'H-v1',\n",
    "      'I-v2': 'I-v1',\n",
    "    }'''\n",
    "\n",
    "    # use a fixed era\n",
    "    model_eradict = {era: 'C-v1'}\n",
    "\n",
    "    # set path\n",
    "    nmf_files = {}\n",
    "    nmf_files[era] = {}\n",
    "    model_era = model_eradict[era]\n",
    "    for layer in layers:\n",
    "        # new convention\n",
    "        nmf_files[era][layer] = os.path.join(modeldir, f'nmf_model_{layer.upper()}_{model_era}.pkl')\n",
    "        # old convention\n",
    "        #nmf_files[era][layer] = os.path.join(modeldir, f'PXLayer_{layer[-1]}', f'nmf_model_era{era}.pkl')\n",
    "    \n",
    "    # existence check\n",
    "    missing = []\n",
    "    for layer, f in nmf_files[era].items():\n",
    "        if not os.path.exists(f): missing.append(f)\n",
    "    if len(missing) > 0:\n",
    "        raise Exception(f'The following files do not exist: {missing}')\n",
    "    \n",
    "\n",
    "    # load models\n",
    "    nmfs = {}\n",
    "    nmfs[era] = {}\n",
    "    for layer in layers:\n",
    "        nmf_file = nmf_files[era][layer]\n",
    "        nmf = joblib.load(nmf_file)\n",
    "        nmfs[era][layer] = nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9474230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preprocessors\n",
    "\n",
    "do_change_preprocessors = False\n",
    "\n",
    "if do_change_preprocessors:\n",
    "    print('Changing preprocessor configuration...')\n",
    "\n",
    "    global_normalization = 'avg'\n",
    "    local_normalization = 'avg_previous_era'\n",
    "    preprocessors = make_preprocessors([era], layers,\n",
    "                                   global_normalization = global_normalization,\n",
    "                                   local_normalization = local_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001f1717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for automasking\n",
    "\n",
    "do_change_automasking = False\n",
    "\n",
    "if do_change_automasking:\n",
    "    print('Changing automask configuration...')\n",
    "\n",
    "    do_automasking = False\n",
    "    automask_data_file = None\n",
    "\n",
    "    if do_automasking:\n",
    "        automask_reader = AutomaskReader(config['automask_data_file'])\n",
    "        automask_map_preprocessors = {}\n",
    "        for layer in layers: automask_map_preprocessors[layer] = PreProcessor(f'PXLayer_{layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for loss masking\n",
    "\n",
    "do_change_loss_masking = False\n",
    "\n",
    "if do_change_loss_masking:\n",
    "    print('Changing loss mask configuration...')\n",
    "    \n",
    "    do_loss_masking = True\n",
    "    zero_frac_threshold = 0.9\n",
    "\n",
    "    if do_loss_masking:\n",
    "        loss_mask_era = era\n",
    "        if '-part' in era: loss_mask_era = era.split('-part')[0]\n",
    "        loss_masks = {}\n",
    "        loss_masks[era] = {}\n",
    "        loss_mask_preprocessors = {}\n",
    "        for layer in layers:\n",
    "            zerofrac_file = f'../preprocessing/normdata/zerofrac_Run2024{loss_mask_era}_{get_metype(layer)}.npy'\n",
    "            zerofrac = np.load(zerofrac_file)\n",
    "            loss_mask = (zerofrac < zero_frac_threshold)\n",
    "            loss_masks[era][layer] = loss_mask\n",
    "            loss_mask_preprocessors[layer] = PreProcessor(f'PXLayer_{layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398ba7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other evaluation settings\n",
    "\n",
    "do_change_evaluation_settings = False\n",
    "\n",
    "if do_change_evaluation_settings:\n",
    "    print('Changing evaluation settings...')\n",
    "\n",
    "    pattern_thresholds = {\n",
    "        #'BPix1': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "        'BPix1': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 24)).tolist(), \"filter_threshold\": 1.5}],\n",
    "        'BPix2': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "        'BPix3': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}],\n",
    "        'BPix4': [{\"loss_threshold\": 0.04, \"pattern\": np.ones((2, 16)).tolist(), \"filter_threshold\": 1.5}]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf208c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load available run and lumisection numbers\n",
    "\n",
    "input_file = input_files[era][layers[0]][0]\n",
    "\n",
    "if isinstance(input_file, str): # older convention\n",
    "    dftemp = iotools.read_parquet(input_file, columns=['run_number', 'ls_number', 'entries'])\n",
    "    dftemp = dftemp[dftemp['entries']>0.5e6]\n",
    "    available_run_numbers = dftemp['run_number'].values\n",
    "    available_ls_numbers = dftemp['ls_number'].values\n",
    "    unique_runs = np.unique(available_run_numbers)\n",
    "\n",
    "elif 'file' in input_file.keys():\n",
    "    dftemp = iotools.read_parquet(input_file['file'], columns=['run_number', 'ls_number', 'entries'])\n",
    "    dftemp = dftemp[dftemp['entries']>0.5e6]\n",
    "    available_run_numbers = dftemp['run_number'].values\n",
    "    available_ls_numbers = dftemp['ls_number'].values\n",
    "    unique_runs = np.unique(available_run_numbers)\n",
    "\n",
    "elif 'dataset' in input_file.keys():\n",
    "    creds = get_dials_creds()\n",
    "    dials = Dials(creds, workspace='tracker')\n",
    "    runfilters = RunFilters(dataset=input_file[\"dataset\"])\n",
    "    runs = dials.run.list_all(runfilters, enable_progress=False).results\n",
    "    unique_runs = sorted([el.run_number for el in runs])\n",
    "    \n",
    "else:\n",
    "    raise Exception(f'Input file {input_file} not recognized.')\n",
    "    \n",
    "print('Available runs:')\n",
    "print(unique_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef09dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for result file\n",
    "\n",
    "# default: use the stored results that match the configuration\n",
    "flagged_ls_file = os.path.join(os.path.dirname(config_file), f'flagged_lumisections_{era}.json')\n",
    "\n",
    "flagged_ls_file_exists = os.path.exists(flagged_ls_file)\n",
    "if not flagged_ls_file_exists:\n",
    "    print(f'WARNING: file {flagged_ls_file} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767834e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load flagged run and lumisection numbers\n",
    "\n",
    "if flagged_ls_file_exists:\n",
    "    \n",
    "    with open(flagged_ls_file, 'r') as f:\n",
    "        info = json.load(f)\n",
    "    flagged_run_numbers = np.array(info['flagged_run_numbers'])\n",
    "    flagged_ls_numbers = np.array(info['flagged_ls_numbers'])\n",
    "    filter_results = info['filter_results']\n",
    "\n",
    "    # printouts\n",
    "    print(f'Found {len(flagged_run_numbers)} flagged lumisections:')\n",
    "    for run_number, ls_number in zip(flagged_run_numbers, flagged_ls_numbers):\n",
    "        print(f'  - Run {run_number}, LS {ls_number}')\n",
    "    \n",
    "    # summary printout\n",
    "    ranges = []\n",
    "    thisrange = [flagged_run_numbers[0], flagged_ls_numbers[0]]\n",
    "    for idx in range(1, len(flagged_run_numbers)):\n",
    "        if flagged_run_numbers[idx] != thisrange[0] or flagged_ls_numbers[idx] != flagged_ls_numbers[idx-1]+1:\n",
    "            thisrange.append(flagged_ls_numbers[idx-1])\n",
    "            ranges.append(thisrange[:])\n",
    "            thisrange = [flagged_run_numbers[idx], flagged_ls_numbers[idx]]\n",
    "            continue\n",
    "        else: continue\n",
    "    thisrange.append(flagged_ls_numbers[-1])\n",
    "    ranges.append(thisrange[:])\n",
    "\n",
    "    print()\n",
    "    print(f'Found {len(flagged_run_numbers)} flagged lumisections:')\n",
    "    for lsrange in ranges:\n",
    "        print(f'  - Run {lsrange[0]}, LS {lsrange[1]} - {lsrange[2]}')\n",
    "    \n",
    "    # even more summary printout\n",
    "    unique_flagged_run_numbers, counts = np.unique(flagged_run_numbers, return_counts=True)\n",
    "    print()\n",
    "    print(f'Found {len(flagged_run_numbers)} flagged lumisections:')\n",
    "    for runnb, count in zip(unique_flagged_run_numbers, counts):\n",
    "        print(f'  - Run {runnb}, {count} lumisections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da189e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a plot of the filter results\n",
    "\n",
    "if flagged_ls_file_exists:\n",
    "    \n",
    "    # make a table\n",
    "    filter_results_arrays = {key: np.array([el[0]*10000+el[1] for el in val]) for key, val in filter_results.items()}\n",
    "    failed_ls = np.unique(np.concatenate(list(filter_results_arrays.values())))\n",
    "    nfiltered = {key: len(val) for key, val in filter_results_arrays.items()}\n",
    "    nfiltered['total'] = len(failed_ls)\n",
    "\n",
    "    # make a figure\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    ax.bar(nfiltered.keys(), nfiltered.values())\n",
    "    ax.set_xticks(ax.get_xticks())\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', fontsize=15)\n",
    "    ax.set_ylabel('Number of failing LS', fontsize=15)\n",
    "    ax.grid(which='both', axis='y', color='gray', linestyle='dashed')\n",
    "    ax.text(0, 1.03, f'Lumisection preselection for era {era}', transform=ax.transAxes, fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c0fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out why a given lumisection did not pass the selections\n",
    "\n",
    "test_runlumis = [(398902, 180)]\n",
    "\n",
    "if flagged_ls_file_exists:\n",
    "\n",
    "    for runlumi in test_runlumis:\n",
    "        failkeys = []\n",
    "        for key, values in filter_results.items():\n",
    "            if list(runlumi) in values: failkeys.append(key)\n",
    "        if len(failkeys)==0:\n",
    "            print(f'Lumisection {runlumi} not found in filter info, i.e. it did not seem to have failed any of the selections.')\n",
    "        else:\n",
    "            print(f'Lumisection {runlumi} failed the following selections:')\n",
    "            print(failkeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d4fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data for plotting some random (or not random) examples\n",
    "\n",
    "# general settings\n",
    "do_extended_loss_plots = True\n",
    "do_combined_loss_plot = True\n",
    "do_combined_loss_mask_plot = False\n",
    "\n",
    "# random lumisections\n",
    "nplot = 0\n",
    "#random_ids = np.random.choice(len(available_run_numbers), size=min(nplot, len(available_run_numbers)), replace=False)\n",
    "#selected_run_numbers = available_run_numbers[random_ids]\n",
    "#selected_ls_numbers = available_ls_numbers[random_ids]\n",
    "#random_ids = np.random.choice(len(flagged_run_numbers), size=min(nplot, len(flagged_run_numbers)), replace=False)\n",
    "#selected_run_numbers = flagged_run_numbers[random_ids]\n",
    "#selected_ls_numbers = flagged_ls_numbers[random_ids]\n",
    "\n",
    "# alternative: specific selected lumisections\n",
    "selected_runlumis = [\n",
    "    (385127, 793)\n",
    "]\n",
    "selected_run_numbers = [el[0] for el in selected_runlumis]\n",
    "selected_ls_numbers = [el[1] for el in selected_runlumis]\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # load data\n",
    "    print('Loading data...')\n",
    "    dfs = {}\n",
    "    mes = {}\n",
    "    for layer in layers:\n",
    "        input_file = input_files[era][layer][0]\n",
    "        if isinstance(input_file, str): # older convention\n",
    "            dfs[layer] = iotools.read_lumisections(input_file, selected_run_numbers, selected_ls_numbers, mode='batched')\n",
    "        elif 'file' in input_file.keys():\n",
    "            dfs[layer] = iotools.read_lumisections(input_file['file'], selected_run_numbers, selected_ls_numbers, mode='batched')\n",
    "        elif 'dataset' in input_file.keys() and 'me' in input_file.keys():\n",
    "            creds = get_dials_creds()\n",
    "            dials = Dials(creds, workspace='tracker')\n",
    "            this_dfs = []\n",
    "            for run, lumi in selected_runlumis:\n",
    "                dialsfilters = LumisectionHistogram2DFilters(\n",
    "                  dataset = input_file[\"dataset\"],\n",
    "                  me = input_file[\"me\"],\n",
    "                  run_number = run,\n",
    "                  ls_number = lumi\n",
    "                )\n",
    "                data = get_data_from_dials(dials, dialsfilters)\n",
    "                this_dfs.append( data.to_pandas() )\n",
    "            dfs[layer] = pd.concat(this_dfs, ignore_index=True)\n",
    "        if len(dfs[layer])==0:\n",
    "            msg = 'WARNING: empty dataframe returned...'\n",
    "            print(msg)\n",
    "        mes[layer], runs, lumis = dftools.get_mes(dfs[layer], xbinscolumn='x_bin', ybinscolumn='y_bin', runcolumn='run_number', lumicolumn='ls_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # preprocess and predict\n",
    "    print('Processing...')\n",
    "    mes_preprocessed = {}\n",
    "    mes_pred = {}\n",
    "    losses = {}\n",
    "    for layer in layers:\n",
    "        mes_preprocessed[layer] = preprocessors[era][layer].preprocess(dfs[layer])\n",
    "        this_mes_preprocessed = np.copy(mes_preprocessed[layer])\n",
    "        # (make a copy for some additional processing before inference\n",
    "        #  that does not need to go in the plot)\n",
    "        if preprocessors is not None:\n",
    "            threshold = 5\n",
    "            this_mes_preprocessed[this_mes_preprocessed > threshold] = threshold\n",
    "        if preprocessors is not None:\n",
    "            this_mes_preprocessed[this_mes_preprocessed == 0] = 1\n",
    "        mes_pred[layer] = nmfs[era][layer].predict(this_mes_preprocessed)\n",
    "        losses[layer] = np.square(mes_preprocessed[layer] - mes_pred[layer])\n",
    "    \n",
    "    # automasking\n",
    "    if do_automasking:\n",
    "        print('Applying automasks...')\n",
    "        for layer in layers:\n",
    "            subsystem = f'BPix{layer}'\n",
    "            automask_maps = automask_reader.get_automask_maps_for_ls(selected_run_numbers, selected_ls_numbers, subsystem, invert=True)\n",
    "            automask_maps = automask_map_preprocessors[layer].preprocess_mes(automask_maps, None, None)\n",
    "            losses[layer] = np.multiply(losses[layer], automask_maps)\n",
    "     \n",
    "    # thresholding\n",
    "    print('Thresholding...')\n",
    "    losses_clustered = {}\n",
    "    for layer in layers:\n",
    "        try: this_pattern_thresholds = pattern_thresholds[layer]\n",
    "        except: this_pattern_thresholds = pattern_thresholds # older convention\n",
    "        losses_clustered[layer] = clustering.cluster_loss_multithreshold(losses[layer], this_pattern_thresholds)\n",
    "    \n",
    "    # make rebinned and overlayed loss map\n",
    "    print('Combining layers...')\n",
    "    target_shape = losses[layers[0]].shape[1:3]\n",
    "    losses_rebinned = {}\n",
    "    losses_combined = np.zeros(losses[layers[0]].shape)\n",
    "    for layer in layers:\n",
    "        losses_rebinned[layer] = rebinning.rebin_keep_clip(losses_clustered[layer], target_shape, 1, mode='cv2')\n",
    "        losses_combined += losses_rebinned[layer]\n",
    "        \n",
    "    # optional: do loss masking\n",
    "    loss_mask = np.zeros(losses_combined.shape)\n",
    "    if do_loss_masking:\n",
    "        print('Applying loss mask...')\n",
    "        loss_mask = np.zeros((1, target_shape[0], target_shape[1]))\n",
    "        for layer in layers:\n",
    "            this_loss_mask = loss_masks[era][layer]\n",
    "            # preprocess\n",
    "            this_loss_mask = np.expand_dims(this_loss_mask, 0)\n",
    "            this_loss_mask = loss_mask_preprocessors[layer].preprocess_mes(this_loss_mask, None, None)\n",
    "            # invert\n",
    "            this_loss_mask = 1 - this_loss_mask\n",
    "            # rescale\n",
    "            this_loss_mask = rebinning.rebin_keep_clip(this_loss_mask, target_shape, 1, mode='cv2')\n",
    "            # add to total\n",
    "            loss_mask += this_loss_mask\n",
    "        loss_mask = np.repeat(loss_mask, len(losses_combined), axis=0)\n",
    "    \n",
    "    # apply threshold on combined loss\n",
    "    losses_final = ((losses_combined >= 2) & (losses_combined > loss_mask)).astype(int)\n",
    "        \n",
    "    # make the plots\n",
    "    print('Plotting...')\n",
    "    for idx in range(len(selected_run_numbers)):\n",
    "        run = runs[idx]\n",
    "        lumi = lumis[idx]\n",
    "        for layer in layers:\n",
    "            me_orig = mes[layer][idx, :, :]\n",
    "            me_preprocessed = mes_preprocessed[layer][idx, :, :]\n",
    "            me_pred = mes_pred[layer][idx, :, :]\n",
    "            loss = losses[layer][idx, :, :]\n",
    "            loss_clustered = losses_clustered[layer][idx, :, :]\n",
    "            loss_rebinned = losses_rebinned[layer][idx, :, :]\n",
    "    \n",
    "            # initialize figure\n",
    "            nrows = 1\n",
    "            figheight = 6\n",
    "            if do_extended_loss_plots:\n",
    "                nrows = 2\n",
    "                figheight = 12\n",
    "            fig, axs = plt.subplots(ncols=4, nrows=nrows, figsize=(24, figheight), squeeze=False)\n",
    "            \n",
    "            # plot raw data\n",
    "            fig, axs[0, 0] = plot_cluster_occupancy(me_orig, fig=fig, ax=axs[0, 0],\n",
    "                   title='Raw', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   caxsoftmax=True,\n",
    "                   docolorbar=True, caxtitle='Number of clusters',\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "        \n",
    "            # overlay automask\n",
    "            if do_automasking:\n",
    "                subsystem = f'BPix{layer}'\n",
    "                automask_map = amreader.get_automask_map_for_ls(run, lumi, subsystem)\n",
    "                ids = np.nonzero(automask_map.astype(int))\n",
    "                for yidx, xidx in zip(ids[0], ids[1]):\n",
    "                    linewidth = 1 if layer>=3 else 2\n",
    "                    patch = mpl.patches.Rectangle((xidx-0.5, yidx-0.5), 1, 1,\n",
    "                                      edgecolor='red', linewidth=linewidth,\n",
    "                                      facecolor='none')\n",
    "                    axs[0, 0].add_patch(patch)\n",
    "        \n",
    "            # plot preprocessed, reconstructed and loss\n",
    "            fig, axs[0, 1] = plot_cluster_occupancy(me_preprocessed, fig=fig, ax=axs[0, 1],\n",
    "                   title='Input', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Number of clusters\\n(normalized)',\n",
    "                   caxrange=(1e-6,2),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            fig, axs[0, 2] = plot_cluster_occupancy(me_pred, fig=fig, ax=axs[0, 2],\n",
    "                   title='Reconstructed', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Number of clusters\\n(normalized)',\n",
    "                   caxrange=(1e-6,2),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            fig, axs[0, 3] = plot_cluster_occupancy(loss, fig=fig, ax=axs[0, 3],\n",
    "                   title='Loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 0.1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            \n",
    "            # optional: plot more post-processing steps with the loss map\n",
    "            if do_extended_loss_plots:\n",
    "                fig, axs[1, 0] = plot_cluster_occupancy(loss_clustered, fig=fig, ax=axs[1, 0],\n",
    "                   title=f'Cleaned loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "                fig, axs[1, 1] = plot_cluster_occupancy(loss_rebinned, fig=fig, ax=axs[1, 1],\n",
    "                   title=f'Rebinned loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "                fig.delaxes(axs[1, 2])\n",
    "                fig.delaxes(axs[1, 3])\n",
    "                \n",
    "            # plot aesthetics\n",
    "            plt.subplots_adjust(wspace=0.55)\n",
    "            if str(layer)=='BPix1': plt.subplots_adjust(hspace=-0.65)\n",
    "            if str(layer)=='BPix2': plt.subplots_adjust(hspace=-0.35)\n",
    "            title = f'Run {run}, LS {lumi}, layer {layer}'\n",
    "            if str(layer)=='BPix1':\n",
    "                axs[0, 0].text(0.01, 1.3, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "            if str(layer)=='BPix2':\n",
    "                axs[0, 0].text(0.01, 1.15, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "            if str(layer)=='BPix3':\n",
    "                axs[0, 0].text(0.01, 1.1, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "            if str(layer)=='BPix4':\n",
    "                axs[0, 0].text(0.01, 1.08, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "        # plot the combined loss map\n",
    "        if do_combined_loss_plot:\n",
    "            fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(12, 6))\n",
    "            # combined loss before loss masking\n",
    "            loss_combined = losses_combined[idx, :, :]\n",
    "            fig, axs[0] = plot_cluster_occupancy(loss_combined, fig=fig, ax=axs[0],\n",
    "                   title='Combined binary loss (before masking)', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss (number of layers)',\n",
    "                   caxrange=(0, 4),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "            # combined loss after loss masking\n",
    "            loss_final = losses_final[idx, :, :]\n",
    "            fig, axs[1] = plot_cluster_occupancy(loss_final, fig=fig, ax=axs[1],\n",
    "                   title='Combined binary loss (after masking)', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss (binary)',\n",
    "                   caxrange=(0, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "            plt.subplots_adjust(wspace=0.55)\n",
    "            title = f'Run {run}, LS {lumi}'\n",
    "            axs[0].text(0.01, 1.2, title, fontsize=15, transform=axs[0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "    # plot the combined loss mask\n",
    "    if do_combined_loss_mask_plot:\n",
    "            loss_mask_combined = loss_mask[0, :, :]\n",
    "            fig, ax = plt.subplots()\n",
    "            fig, ax = plot_cluster_occupancy(loss_mask_combined, fig=fig, ax=ax,\n",
    "                   title='Combined loss mask', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Mask (number of layers)',\n",
    "                   caxrange=(0, 4),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "            title = f'Run {run}, LS {lumi}'\n",
    "            ax.text(0.01, 1.3, title, fontsize=15, transform=ax.transAxes)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3030121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example workfow\n",
    "# (simplified and prettified version for DP note)\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # preprocess and predict\n",
    "    print('Processing...')\n",
    "    mes_preprocessed = {}\n",
    "    mes_pred = {}\n",
    "    losses = {}\n",
    "    for layer in layers:\n",
    "        mes_preprocessed[layer] = preprocessors[era][layer].preprocess(dfs[layer])\n",
    "        this_mes_preprocessed = np.copy(mes_preprocessed[layer])\n",
    "        # (make a copy for some additional processing before inference\n",
    "        #  that does not need to go in the plot)\n",
    "        if preprocessors is not None:\n",
    "            threshold = 5\n",
    "            this_mes_preprocessed[this_mes_preprocessed > threshold] = threshold\n",
    "        if preprocessors is not None:\n",
    "            this_mes_preprocessed[this_mes_preprocessed == 0] = 1\n",
    "        mes_pred[layer] = nmfs[era][layer].predict(this_mes_preprocessed)\n",
    "        losses[layer] = np.square(mes_preprocessed[layer] - mes_pred[layer])\n",
    "     \n",
    "    # thresholding\n",
    "    print('Thresholding...')\n",
    "    losses_clustered = {}\n",
    "    for layer in layers:\n",
    "        try: this_pattern_thresholds = pattern_thresholds[layer]\n",
    "        except: this_pattern_thresholds = pattern_thresholds # older convention\n",
    "        losses_clustered[layer] = clustering.cluster_loss_multithreshold(losses[layer], this_pattern_thresholds)\n",
    "        \n",
    "    # make the plots\n",
    "    print('Plotting...')\n",
    "    for idx in range(len(selected_run_numbers)):\n",
    "        run = runs[idx]\n",
    "        lumi = lumis[idx]\n",
    "        for layer in layers[:1]: # plot only layer 1\n",
    "            me_orig = mes[layer][idx, :, :]\n",
    "            me_preprocessed = mes_preprocessed[layer][idx, :, :]\n",
    "            me_pred = mes_pred[layer][idx, :, :]\n",
    "            loss = losses[layer][idx, :, :]\n",
    "            loss_clustered = losses_clustered[layer][idx, :, :]\n",
    "    \n",
    "            # initialize figure\n",
    "            nrows = 2\n",
    "            figheight = 12\n",
    "            fig, axs = plt.subplots(ncols=3, nrows=nrows, figsize=(18, figheight), squeeze=False)\n",
    "            \n",
    "            # plot raw data\n",
    "            fig, axs[0, 0] = plot_cluster_occupancy(me_orig, fig=fig, ax=axs[0, 0],\n",
    "                   title='Raw', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   caxsoftmax=True,\n",
    "                   docolorbar=True, caxtitle='Number of clusters',\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "            fig, axs[0, 1] = plot_cluster_occupancy(me_preprocessed, fig=fig, ax=axs[0, 1],\n",
    "                   title='Input', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Number of clusters\\n(normalized)',\n",
    "                   caxrange=(1e-6,2),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            fig, axs[1, 0] = plot_cluster_occupancy(me_pred, fig=fig, ax=axs[1, 0],\n",
    "                   title='Reconstructed', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Number of clusters\\n(normalized)',\n",
    "                   caxrange=(1e-6,2),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            fig, axs[1, 1] = plot_cluster_occupancy(loss, fig=fig, ax=axs[1, 1],\n",
    "                   title='Loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 0.1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30)\n",
    "            fig, axs[1, 2] = plot_cluster_occupancy(loss_clustered, fig=fig, ax=axs[1, 2],\n",
    "                   title=f'Cleaned loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 1),\n",
    "                   caxtitlesize=15, caxtitleoffset=15)\n",
    "            fig.delaxes(axs[0, 2])\n",
    "                \n",
    "            # plot aesthetics\n",
    "            if str(layer)=='BPix1':\n",
    "                plt.subplots_adjust(hspace=-0.7)\n",
    "                plt.subplots_adjust(wspace=0.4)\n",
    "            if str(layer)=='BPix2': plt.subplots_adjust(hspace=0.3)\n",
    "            if str(layer)=='BPix3': plt.subplots_adjust(hspace=0.3)\n",
    "            if str(layer)=='BPix4': plt.subplots_adjust(hspace=0.3)\n",
    "            title = r'$\\bf{CMS}$ ' + r'$\\it{Preliminary}$'\n",
    "            year = era[:4]\n",
    "            if not year.startswith('202'): year = '2024' # older convention\n",
    "            conditions = f'{layer}, {year} (13.6 TeV)'\n",
    "            if str(layer)=='BPix1':\n",
    "                axs[0, 0].text(0.01, 1.4, title, fontsize=15, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.25, conditions, fontsize=15, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix2':\n",
    "                axs[0, 0].text(0.01, 1.15, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.1, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix3':\n",
    "                axs[0, 0].text(0.01, 1.15, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.1, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix4':\n",
    "                axs[0, 0].text(0.01, 1.14, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.09, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7a16d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot examples of raw + loss\n",
    "# (simplified and prettified version for DP note)\n",
    "\n",
    "import studies.pixel_clusters_2024.plotting.plot_cluster_occupancy\n",
    "importlib.reload(studies.pixel_clusters_2024.plotting.plot_cluster_occupancy)\n",
    "from studies.pixel_clusters_2024.plotting.plot_cluster_occupancy import plot_cluster_occupancy\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # preprocess and predict\n",
    "    print('Processing...')\n",
    "    mes_preprocessed = {}\n",
    "    mes_pred = {}\n",
    "    losses = {}\n",
    "    for layer in layers:\n",
    "        mes_preprocessed[layer] = preprocessors[era][layer].preprocess(dfs[layer])\n",
    "        this_mes_preprocessed = np.copy(mes_preprocessed[layer])\n",
    "        # (make a copy for some additional processing before inference\n",
    "        #  that does not need to go in the plot)\n",
    "        if preprocessors is not None:\n",
    "            threshold = 5\n",
    "            this_mes_preprocessed[this_mes_preprocessed > threshold] = threshold\n",
    "        if preprocessors is not None:\n",
    "            this_mes_preprocessed[this_mes_preprocessed == 0] = 1\n",
    "        mes_pred[layer] = nmfs[era][layer].predict(this_mes_preprocessed)\n",
    "        losses[layer] = np.square(mes_preprocessed[layer] - mes_pred[layer])\n",
    "        \n",
    "    # make the plots\n",
    "    print('Plotting...')\n",
    "    for idx in range(len(selected_run_numbers)):\n",
    "        run = runs[idx]\n",
    "        lumi = lumis[idx]\n",
    "        for layer in layers:\n",
    "            me_orig = mes[layer][idx, :, :]\n",
    "            me_preprocessed = mes_preprocessed[layer][idx, :, :]\n",
    "            me_pred = mes_pred[layer][idx, :, :]\n",
    "            loss = losses[layer][idx, :, :]\n",
    "    \n",
    "            # initialize figure\n",
    "            figheight = 6\n",
    "            fig, axs = plt.subplots(ncols=1, nrows=2, figsize=(6, 10), squeeze=False)\n",
    "            \n",
    "            # plot raw data\n",
    "            fig, axs[0, 0] = plot_cluster_occupancy(me_orig, fig=fig, ax=axs[0, 0],\n",
    "                   title='Raw', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   caxsoftmax=True,\n",
    "                   docolorbar=True, caxtitle='Number of clusters',\n",
    "                   caxtitlesize=15, caxtitleoffset=15,\n",
    "            )\n",
    "        \n",
    "        \n",
    "            # plot loss\n",
    "            fig, axs[1, 0] = plot_cluster_occupancy(loss, fig=fig, ax=axs[1, 0],\n",
    "                   title='Loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 0.1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30,\n",
    "            )\n",
    "                \n",
    "            # plot aesthetics\n",
    "            if str(layer)=='BPix1': plt.subplots_adjust(hspace=-0.45)\n",
    "            if str(layer)=='BPix2': plt.subplots_adjust(hspace=0.3)\n",
    "            if str(layer)=='BPix3': plt.subplots_adjust(hspace=0.3)\n",
    "            if str(layer)=='BPix4': plt.subplots_adjust(hspace=0.3)\n",
    "            title = r'$\\bf{CMS}$ ' + r'$\\it{Preliminary}$'\n",
    "            year = era[:4]\n",
    "            if not year.startswith('202'): year = '2024' # older convention\n",
    "            conditions = f'{layer}, {year} (13.6 TeV)'\n",
    "            if str(layer)=='BPix1':\n",
    "                axs[0, 0].text(0.01, 1.35, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.25, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix2':\n",
    "                axs[0, 0].text(0.01, 1.15, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.1, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix3':\n",
    "                axs[0, 0].text(0.01, 1.15, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.1, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            if str(layer)=='BPix4':\n",
    "                axs[0, 0].text(0.01, 1.14, title, fontsize=12, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.09, conditions, fontsize=12, transform=axs[0,0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d738d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but with different aspect ratio\n",
    "\n",
    "import studies.pixel_clusters_2024.plotting.plot_cluster_occupancy\n",
    "importlib.reload(studies.pixel_clusters_2024.plotting.plot_cluster_occupancy)\n",
    "from studies.pixel_clusters_2024.plotting.plot_cluster_occupancy import plot_cluster_occupancy\n",
    "\n",
    "if len(selected_run_numbers) > 0:\n",
    "    \n",
    "    # preprocess and predict\n",
    "    print('Processing...')\n",
    "    mes_preprocessed = {}\n",
    "    mes_pred = {}\n",
    "    losses = {}\n",
    "    for layer in layers:\n",
    "        mes_preprocessed[layer] = preprocessors[era][layer].preprocess(dfs[layer])\n",
    "        this_mes_preprocessed = np.copy(mes_preprocessed[layer])\n",
    "        # (make a copy for some additional processing before inference\n",
    "        #  that does not need to go in the plot)\n",
    "        if preprocessors is not None:\n",
    "            threshold = 5\n",
    "            this_mes_preprocessed[this_mes_preprocessed > threshold] = threshold\n",
    "        if preprocessors is not None:\n",
    "            this_mes_preprocessed[this_mes_preprocessed == 0] = 1\n",
    "        mes_pred[layer] = nmfs[era][layer].predict(this_mes_preprocessed)\n",
    "        losses[layer] = np.square(mes_preprocessed[layer] - mes_pred[layer])\n",
    "        \n",
    "    # make the plots\n",
    "    print('Plotting...')\n",
    "    for idx in range(len(selected_run_numbers)):\n",
    "        run = runs[idx]\n",
    "        lumi = lumis[idx]\n",
    "        for layer in layers:\n",
    "            me_orig = mes[layer][idx, :, :]\n",
    "            me_preprocessed = mes_preprocessed[layer][idx, :, :]\n",
    "            me_pred = mes_pred[layer][idx, :, :]\n",
    "            loss = losses[layer][idx, :, :]\n",
    "            \n",
    "            aspect = 'equal' if layer=='BPix1' else 'auto'\n",
    "    \n",
    "            # initialize figure\n",
    "            figheight = 6\n",
    "            fig, axs = plt.subplots(ncols=1, nrows=2, figsize=(6, 10), squeeze=False)\n",
    "            \n",
    "            # plot raw data\n",
    "            fig, axs[0, 0] = plot_cluster_occupancy(me_orig, fig=fig, ax=axs[0, 0],\n",
    "                   title='Raw', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   caxsoftmax=True,\n",
    "                   docolorbar=True, caxtitle='Number of clusters',\n",
    "                   caxtitlesize=15, caxtitleoffset=15,\n",
    "                   aspect=aspect\n",
    "            )\n",
    "        \n",
    "        \n",
    "            # plot loss\n",
    "            fig, axs[1, 0] = plot_cluster_occupancy(loss, fig=fig, ax=axs[1, 0],\n",
    "                   title='Loss', titlesize=15,\n",
    "                   xaxtitlesize=15, yaxtitlesize=15,\n",
    "                   ticklabelsize=12, colorticklabelsize=12,\n",
    "                   docolorbar=True, caxtitle='Loss',\n",
    "                   caxrange=(0, 0.1),\n",
    "                   caxtitlesize=15, caxtitleoffset=30,\n",
    "                   aspect=aspect\n",
    "            )\n",
    "                \n",
    "            # plot aesthetics\n",
    "            if str(layer)=='BPix1': plt.subplots_adjust(hspace=-0.45)\n",
    "            else: plt.subplots_adjust(hspace=0.3)\n",
    "            title = r'$\\bf{CMS}$ ' + r'$\\it{Preliminary}$'\n",
    "            year = era[:4]\n",
    "            if not year.startswith('202'): year = '2024' # older convention\n",
    "            conditions = f'{layer}, {year} (13.6 TeV)'\n",
    "            if str(layer)=='BPix1':\n",
    "                axs[0, 0].text(0.01, 1.4, title, fontsize=15, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.25, conditions, fontsize=15, transform=axs[0,0].transAxes)\n",
    "            else:\n",
    "                axs[0, 0].text(0.01, 1.22, title, fontsize=15, transform=axs[0, 0].transAxes)\n",
    "                axs[0, 0].text(0.01, 1.15, conditions, fontsize=15, transform=axs[0,0].transAxes)\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70736d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
